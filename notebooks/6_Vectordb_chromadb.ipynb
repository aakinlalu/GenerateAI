{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/qa_intro-9b468dbffe1cbe7f0bd822b28648db9e.png)\n",
    "## Steps\n",
    "1. Loading: We load unstructured data from web as a document.\n",
    "2. Splitting: Break the document into splits of specific sizes.\n",
    "3. Storage:  Embed the splits and store it into vector database.\n",
    "4. Retrieval: The app retrieves splits from storage.\n",
    "5. Generation: An LLM produces an answer using a prompt that includes the question and the retrieved data\n",
    "6. Conversation: Hold a multi-turn conversation by adding Memory to your QA chain.\n",
    "![](images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import dotenv\n",
    "import logging \n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "from chromadb_cli.chromadb_cli import load_doc_to_db, load_n_split_doc, get_db\n",
    "\n",
    "logging.basicConfig() \n",
    "\n",
    "collection_name = \"airmaster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# client = chromadb.HttpClient(settings=Settings(allow_reset=True))\n",
    "# client.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1, 2, 3: Load, Split and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = 'sentence-transformers/all-mpnet-base-v2'\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "#Emebdding model\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=embedding_model, model_kwargs=model_kwargs)\n",
    "\n",
    "# document\n",
    "http_path = \"https://www.airmaster.com.au/who-we-are\"\n",
    "\n",
    "# load and split the documents\n",
    "docs = load_n_split_doc(http_path, 500)\n",
    "# load the documents to the database\n",
    "vectorstore = load_doc_to_db(documents=docs, collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Retrieve\n",
    "Using chroma client to access the database `chromadb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = get_db(collection_name=collection_name)\n",
    "\n",
    "question = \"When airmaster was founded?\"\n",
    "result = vectorstore.similarity_search(question, k=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.retrievers import SVMRetriever\n",
    "\n",
    "# svm_retriever = SVMRetriever.from_documents(docs, hf_embeddings)\n",
    "# svm_retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generation\n",
    "Distill the retrieved documents into an answer using an LLM/Chat model\n",
    "\n",
    "1. MultiQueryRetriever generates variants of the input question to improve retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ./models/llama-2-7b-chat.ggmlv3.q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[89103]: Class GGMLMetalClass is implemented in both /Users/adebayoakinlalu/.pyenv/versions/3.10.12/envs/llm-env/lib/python3.10/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x2997e8208) and /Users/adebayoakinlalu/.pyenv/versions/3.10.12/envs/llm-env/lib/python3.10/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x299b5c208). One of the two will be used. Which one is undefined.\n",
      "llama.cpp: using Metal\n",
      "llama.cpp: loading model from ./models/llama-2-7b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
      "llama_model_load_internal: mem required  = 5407.71 MB (+ 1026.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1024.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/adebayoakinlalu/.pyenv/versions/3.10.12/envs/llm-env/lib/python3.10/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x298366e90\n",
      "ggml_metal_init: loaded kernel_mul                            0x2983683a0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x298369920\n",
      "ggml_metal_init: loaded kernel_scale                          0x298368600\n",
      "ggml_metal_init: loaded kernel_silu                           0x298368860\n",
      "ggml_metal_init: loaded kernel_relu                           0x29836a2e0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x29836ac50\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x29836bd30\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x29836d150\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x29836bf90\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x29836c590\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x29836efd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x29836e630\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x29836dc90\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x29836fac0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x298370470\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x298370df0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x298371770\n",
      "ggml_metal_init: loaded kernel_norm                           0x2983720f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x298372b70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x298373e60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x298374280\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x298374c50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x2983756f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x298376120\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x298376cb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x2983776d0\n",
      "ggml_metal_init: loaded kernel_rope                           0x298378800\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x298379510\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x29837a140\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x29837ad10\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x29837b8c0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 10922.67 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3616.08 MB, ( 3616.47 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB, ( 4384.47 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1026.00 MB, ( 5410.47 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB, ( 5922.47 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB, ( 6434.47 / 10922.67)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: max tensor size =    70.31 MB\n",
      "\n",
      "1. What are the key events in Airmaster's history that led up to its founding?\n",
      "2. Which companies or organizations were involved in the development of Airmaster technology before its founding?\n",
      "3. How did Airmaster differentiate itself from other similar technologies when it was first introduced?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: [\"1. What are the key events in Airmaster's history that led up to its founding?\", '2. Which companies or organizations were involved in the development of Airmaster technology before its founding?', '3. How did Airmaster differentiate itself from other similar technologies when it was first introduced?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\u200b\\n\\u200bAirmaster’s commitment to sustainability is achieved through a proactive, integrated approach to helping organisations achieve energy efficiency in innovative ways.\\n\\xa0\\nAirmaster officially opened for business on 2nd February 1988 from a small factory in Ferntree Gully. With an emphasis on service and relationships, Airmaster quickly attained high profile clients like Highpoint Shopping Centre and The Age building in Spencer Street.\\n\\u200b', metadata={'language': 'en', 'source': 'https://www.airmaster.com.au/who-we-are', 'title': 'Who We Are | Airmaster'}),\n",
       " Document(page_content='Who We Are | Airmaster\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ntop of page', metadata={'language': 'en', 'source': 'https://www.airmaster.com.au/who-we-are', 'title': 'Who We Are | Airmaster'}),\n",
       " Document(page_content='\\u200b\\nSince its inception, Airmaster has built a winning culture based on employee retention and a deliver-at-all-costs attitude.\\n\\xa0\\nAirmaster’s apprentices and young staff members are among the most talented in the industry, and continue to be recognised through awards such as Box Hill TAFEs Apprentice of the Year and the ARBS Young Achiever awards.\\n\\u200b\\nAs well as its ongoing commitment to employing apprentices, Airmaster has long been an industry leader in workplace gender equality.\\n\\u200b', metadata={'language': 'en', 'source': 'https://www.airmaster.com.au/who-we-are', 'title': 'Who We Are | Airmaster'}),\n",
       " Document(page_content='After acquiring a deal to provide service and maintenance to the nine-level Myer Centre Brisbane, Airmaster opened a first interstate branch in Queensland in early 2000. In 2002, Airmaster established its first permanent office in New South Wales and today has a total of 14 branches across Australia and New Zealand.\\xa0\\n\\u200b', metadata={'language': 'en', 'source': 'https://www.airmaster.com.au/who-we-are', 'title': 'Who We Are | Airmaster'}),\n",
       " Document(page_content='Celebrating its 30th Anniversary in 2018, Airmaster has grown to have 14 branches around Australia and New Zealand. Airmaster is an industry leader with a 1100+ strong workforce made up of technicians, apprentices, engineering and support staff.\\n\\u200b\\nA geographically and technically-diverse workforce means Airmaster is able to respond quickly to customer requests, and being a single source technical services company means that when customers engage Airmaster, they only deal with Airmaster people.', metadata={'language': 'en', 'source': 'https://www.airmaster.com.au/who-we-are', 'title': 'Who We Are | Airmaster'}),\n",
       " Document(page_content='Where once Airmaster predominantly carried out traditional prescribed service and maintenance of HVAC systems, modern advances in information technology, energy management, controls, automation and asset management now drive the company’s integrated service delivery model.\\u200b', metadata={'language': 'en', 'source': 'https://www.airmaster.com.au/who-we-are', 'title': 'Who We Are | Airmaster'}),\n",
       " Document(page_content='IntegrationSchneider EcoXpertOur ProjectsOur SolutionsAsset ManagementUltravioletPlant Room OptimisationIntelligent MaintenanceBuilding Services NetworkCOVID-19 SolutionsSafeSpaceNewsMoreUse tab to navigate through the menu items.who we are.\\xa0Airmaster is an award-winning building services company, providing HVAC&R management, smart building solutions and fire services across Australia, New Zealand and South-East Asia\\u200b', metadata={'language': 'en', 'source': 'https://www.airmaster.com.au/who-we-are', 'title': 'Who We Are | Airmaster'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "callback_handler = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "llm = GPT4All(model=\"./models/llama-2-7b-chat.ggmlv3.q4_0.bin\", callbacks=callback_handler, verbose=True)\n",
    "\n",
    "retriever_from_llm= MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(), llm=llm)\n",
    "\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "unique_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. RetrievakQA is used to chain for  question-answering against an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the text, we know that Airmaster was officially opened for business on February 2nd, 1988, so the answer is 1988."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'When airmaster was founded?',\n",
       " 'result': ' Based on the text, we know that Airmaster was officially opened for business on February 2nd, 1988, so the answer is 1988.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retrieval_qa = RetrievalQA.from_chain_type(retriever=vectorstore.as_retriever(), llm=llm)\n",
    "retrieval_qa({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Customizing the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Airmaster was founded on 2nd February 1988."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'When airmaster was founded?',\n",
       " 'result': ' Airmaster was founded on 2nd February 1988.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template =\"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum and keep the answer as concise as possible. \n",
    "Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT= PromptTemplate.from_template(template)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    retriever=vectorstore.as_retriever(), \n",
    "    llm=llm,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "    )\n",
    "\n",
    "qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Customizing retrieved document processing\n",
    "Retrieved documents can be fed to an LLM for answer distillation in a few different ways.\n",
    "\n",
    "stuff, refine, map-reduce, and map-rerank chains for passing documents to an LLM prompt are well summarized here.\n",
    "\n",
    "stuff is commonly used because it simply \"stuffs\" all retrieved documents into the prompt.\n",
    "\n",
    "The load_qa_chain is an easy way to pass documents to an LLM using these various approaches (e.g., see chain_type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the text provided, Airmaster was founded in 1988."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output_text': ' Based on the text provided, Airmaster was founded in 1988.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "chain({\"input_documents\": unique_docs, \"question\": question}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the text, we know that Airmaster was officially opened for business on February 2nd, 1988, so the answer is 1988."
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Based on the text, we know that Airmaster was officially opened for business on February 2nd, 1988, so the answer is 1988.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=vectorstore.as_retriever(), memory=memory)\n",
    "\n",
    "result = qa({\"question\": question})\n",
    "\n",
    "result[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Can you tell me about the key events in Airmaster's history leading up to its founding in 1988? Based on the provided text, I can see that Airmaster was officially opened for business on February 2nd, 1988 from a small factory in Ferntree Gully. However, I cannot find any information about key events leading up to its founding. Therefore, I must answer \"I don't know\" to your question."
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Based on the provided text, I can see that Airmaster was officially opened for business on February 2nd, 1988 from a small factory in Ferntree Gully. However, I cannot find any information about key events leading up to its founding. Therefore, I must answer \"I don\\'t know\" to your question.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question2 = \"What are the key events in Airmaster's history that led up to its founding?\"\n",
    "\n",
    "result = qa({\"question\": question2})\n",
    "\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Who developed Airmaster technology before its founding? I don't know."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I don't know.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question3= \"Which companies or organizations were involved in the development of Airmaster technology before its founding?\"\n",
    "\n",
    "result = qa({\"question\": question3})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "HF_AUTH_TOKEN = os.getenv(\"HF_AUTH_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture = \"allenai/unifiedqa-t5-small\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# model = T5ForConditionalGeneration.from_pretrained(architecture)\n",
    "# tokenizer = T5Tokenizer.from_pretrained(architecture)\n",
    "\n",
    "# def run_model(input_string, **generator_args):\n",
    "#     input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "#     res = model.generate(input_ids, **generator_args)\n",
    "#     return tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     'text-generation', \n",
    "#      model=model, \n",
    "#      tokenizer=tokenizer,\n",
    "#      max_length=100,\n",
    "#      temperature=0,\n",
    "#      top_p=0.95,\n",
    "#     repetition_penalty=1.2,\n",
    "#     use_auth_token=HF_AUTH_TOKEN\n",
    "#      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it condensed.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model(\"scott filled a tray with juice and put it in a freezer. the next day, scott opened the freezer. how did the juice most likely change? \\\\n (a) it condensed. (b) it evaporated. (c) it became a gas. (d) it became a solid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iron']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model(\"which is best conductor? \\\\n (a) iron (b) feather (c) wood (d) plastic\",\n",
    "         temperature=0.9, num_beams=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Settings(environment='', chroma_db_impl=None, chroma_api_impl='chromadb.api.fastapi.FastAPI', chroma_telemetry_impl='chromadb.telemetry.posthog.Posthog', chroma_sysdb_impl='chromadb.db.impl.sqlite.SqliteDB', chroma_producer_impl='chromadb.db.impl.sqlite.SqliteDB', chroma_consumer_impl='chromadb.db.impl.sqlite.SqliteDB', chroma_segment_manager_impl='chromadb.segment.impl.manager.local.LocalSegmentManager', tenant_id='default', topic_namespace='default', is_persistent=False, persist_directory='./chroma', chroma_server_host='localhost', chroma_server_headers={}, chroma_server_http_port=8000, chroma_server_ssl_enabled=False, chroma_server_grpc_port=None, chroma_server_cors_allow_origins=[], chroma_server_auth_provider=None, chroma_server_auth_configuration_provider=None, chroma_server_auth_configuration_file=None, chroma_server_auth_credentials_provider=None, chroma_server_auth_credentials_file=None, chroma_server_auth_credentials=None, chroma_client_auth_provider=None, chroma_server_auth_ignore_paths={'/api/v1': ['GET'], '/api/v1/heartbeat': ['GET'], '/api/v1/version': ['GET']}, chroma_client_auth_credentials_provider='chromadb.auth.providers.ConfigurationClientAuthCredentialsProvider', chroma_client_auth_protocol_adapter='chromadb.auth.providers.RequestsClientAuthProtocolAdapter', chroma_client_auth_credentials_file=None, chroma_client_auth_credentials=None, anonymized_telemetry=True, allow_reset=False, migrations='apply')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
