{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use litGPT for end to end LLM Operations\n",
    "Use different libraries for LLM operation can be overwhelmed. Your objective is to use one library for different LLM operation, this article is you.\n",
    "\n",
    "LitGPT is an handy open-source library designed to simplify the process of training, fine-tuning, and deploying large language models. It provides tools to download, prepare, and interact with models through a command-line interface, making it easier to incorporate state-of-the-art NLP capabilities into various projects. It is licenced as Apache 2.0.\n",
    "\n",
    "LitGPT was developed by the Lightning AI team, whose mission is to democratize and streamline the process of building and deploying advanced machine learning models.\n",
    "\n",
    "litGPT can be used as a standolone on your GPU. However it is tightly integrated to Lightning Studio.\n",
    "\n",
    "The library has Python API and CLI. We will explore CLI in this article:\n",
    "\n",
    "The CLI functionalities/actions include\n",
    "\n",
    "* Downloads large language models  with `litgpt dowmload <<LLM>>`\n",
    "\n",
    "* Finetunes models on custom datasets, use\n",
    "   ```bash\n",
    "   litgpt finetune <<LLM>>  \\\n",
    "   --data JSON \\\n",
    "   --data.json_path custom_dataset.json \\ # Your custom dataset\n",
    "   --data.val_split_fraction 0.1 \\  # Split the dataset into training set and validation data with 9:1 ratio\n",
    "   --out_dir out/custom-model  # Location custom model would be stored. It creates the directory if it does not exists\n",
    "    ```\n",
    " To explore the cli option, use `litgpt finetune --help`\n",
    "\n",
    "* Evaluate the model with  `litgpt evaluate out/custom-model --task <<TASK>>`\n",
    "\n",
    "* Chat with the model with `litgpt chat out/custom-model`\n",
    "\n",
    "* Generate using the model `litgpt generate out/custom-model --prompt ...`\n",
    "\n",
    "* Deploy the model with `litgpt serve out/custom-model`\n",
    "\n",
    "Optional\n",
    "* Pretrain your own model with large dataset using `litgpt pretrain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'litgpt[all]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 5 files:   0%|                                   | 0/5 [00:00<?, ?it/s]\n",
      "tokenizer.json:   0%|                               | 0.00/9.09M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model.safetensors:   0%|                            | 0.00/2.47G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "tokenizer_config.json:   0%|                        | 0.00/54.5k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "config.json: 100%|█████████████████████████████| 877/877 [00:00<00:00, 5.74MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching 5 files:  20%|█████▍                     | 1/5 [00:01<00:05,  1.43s/it]\n",
      "\n",
      "\n",
      "\n",
      "generation_config.json: 100%|██████████████████| 189/189 [00:00<00:00, 1.56MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "tokenizer_config.json: 100%|███████████████| 54.5k/54.5k [00:00<00:00, 1.22MB/s]\n",
      "\n",
      "\n",
      "model.safetensors:   0%|                   | 10.5M/2.47G [00:02<09:50, 4.17MB/s]\u001b[A\u001b[A\n",
      "tokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:03<00:00, 2.50MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:   1%|▏                  | 21.0M/2.47G [00:04<08:24, 4.86MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   1%|▏                  | 31.5M/2.47G [00:05<07:16, 5.59MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   2%|▎                  | 41.9M/2.47G [00:07<06:43, 6.02MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   2%|▍                  | 52.4M/2.47G [00:09<06:25, 6.28MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   3%|▍                  | 62.9M/2.47G [00:10<06:13, 6.46MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   3%|▌                  | 73.4M/2.47G [00:12<06:05, 6.57MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   3%|▋                  | 83.9M/2.47G [00:13<06:13, 6.39MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   4%|▋                  | 94.4M/2.47G [00:15<06:05, 6.50MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   4%|▊                   | 105M/2.47G [00:16<05:59, 6.58MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   5%|▉                   | 115M/2.47G [00:18<05:54, 6.65MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   5%|█                   | 126M/2.47G [00:20<05:50, 6.70MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   6%|█                   | 136M/2.47G [00:21<05:46, 6.73MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   6%|█▏                  | 147M/2.47G [00:23<05:44, 6.75MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   6%|█▎                  | 157M/2.47G [00:24<05:54, 6.53MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   7%|█▎                  | 168M/2.47G [00:26<05:48, 6.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   7%|█▍                  | 178M/2.47G [00:27<05:43, 6.67MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   8%|█▌                  | 189M/2.47G [00:29<05:40, 6.71MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   8%|█▌                  | 199M/2.47G [00:31<05:38, 6.72MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   8%|█▋                  | 210M/2.47G [00:32<05:35, 6.75MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   9%|█▊                  | 220M/2.47G [00:34<05:45, 6.52MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   9%|█▊                  | 231M/2.47G [00:35<05:39, 6.60MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  10%|█▉                  | 241M/2.47G [00:37<05:34, 6.67MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  10%|██                  | 252M/2.47G [00:38<05:30, 6.71MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  11%|██                  | 262M/2.47G [00:40<05:27, 6.74MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  11%|██▏                 | 273M/2.47G [00:41<05:25, 6.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  11%|██▎                 | 283M/2.47G [00:43<05:22, 6.78MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  12%|██▍                 | 294M/2.47G [00:45<05:33, 6.54MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  12%|██▍                 | 304M/2.47G [00:46<05:27, 6.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  13%|██▌                 | 315M/2.47G [00:48<05:24, 6.65MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  13%|██▋                 | 325M/2.47G [00:49<05:20, 6.70MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  14%|██▋                 | 336M/2.47G [00:51<05:17, 6.73MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  14%|██▊                 | 346M/2.47G [00:52<05:14, 6.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  14%|██▉                 | 357M/2.47G [00:54<05:13, 6.74MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  15%|██▉                 | 367M/2.47G [00:56<05:11, 6.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  15%|███                 | 377M/2.47G [00:57<05:09, 6.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  16%|███▏                | 388M/2.47G [00:59<05:07, 6.79MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  16%|███▏                | 398M/2.47G [01:00<05:05, 6.79MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  17%|███▎                | 409M/2.47G [01:02<05:05, 6.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  17%|███▍                | 419M/2.47G [01:03<05:02, 6.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  17%|███▍                | 430M/2.47G [01:05<05:12, 6.54MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  18%|███▌                | 440M/2.47G [01:07<05:06, 6.62MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  18%|███▋                | 451M/2.47G [01:08<05:02, 6.68MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  19%|███▋                | 461M/2.47G [01:10<05:00, 6.70MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  19%|███▊                | 472M/2.47G [01:11<04:56, 6.74MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  20%|███▉                | 482M/2.47G [01:13<04:54, 6.75MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  20%|███▉                | 493M/2.47G [01:14<05:03, 6.52MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  20%|████                | 503M/2.47G [01:16<04:57, 6.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  21%|████▏               | 514M/2.47G [01:18<04:53, 6.67MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  21%|████▏               | 524M/2.47G [01:19<04:50, 6.70MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  22%|████▎               | 535M/2.47G [01:21<04:47, 6.74MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  22%|████▍               | 545M/2.47G [01:22<04:44, 6.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  22%|████▍               | 556M/2.47G [01:24<04:42, 6.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  23%|████▌               | 566M/2.47G [01:25<04:51, 6.54MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  23%|████▋               | 577M/2.47G [01:27<04:46, 6.62MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  24%|████▊               | 587M/2.47G [01:29<04:42, 6.68MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  24%|████▊               | 598M/2.47G [01:30<04:39, 6.72MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  25%|████▉               | 608M/2.47G [01:32<04:36, 6.74MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  25%|█████               | 619M/2.47G [01:33<04:34, 6.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  25%|█████               | 629M/2.47G [01:35<04:42, 6.53MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  26%|█████▏              | 640M/2.47G [01:36<04:37, 6.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  26%|█████▎              | 650M/2.47G [01:38<04:33, 6.67MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  27%|█████▎              | 661M/2.47G [01:40<04:30, 6.69MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  27%|█████▍              | 671M/2.47G [01:41<04:27, 6.73MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  28%|█████▌              | 682M/2.47G [01:43<04:24, 6.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  28%|█████▌              | 692M/2.47G [01:44<04:22, 6.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  28%|█████▋              | 703M/2.47G [01:46<04:30, 6.54MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  29%|█████▊              | 713M/2.47G [01:47<04:26, 6.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  29%|█████▊              | 724M/2.47G [01:49<04:22, 6.66MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  30%|█████▉              | 734M/2.47G [01:51<04:20, 6.68MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  30%|██████              | 744M/2.47G [01:52<04:16, 6.72MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  31%|██████              | 755M/2.47G [01:54<04:14, 6.75MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  31%|██████▏             | 765M/2.47G [01:56<04:45, 5.98MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  31%|██████▎             | 776M/2.47G [01:57<04:38, 6.08MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  32%|██████▎             | 786M/2.47G [01:59<04:28, 6.28MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  32%|██████▍             | 797M/2.47G [02:01<04:40, 5.98MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  33%|██████▌             | 807M/2.47G [02:03<04:28, 6.20MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  33%|██████▌             | 818M/2.47G [02:04<04:19, 6.38MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  34%|██████▋             | 828M/2.47G [02:06<04:26, 6.16MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  34%|██████▊             | 839M/2.47G [02:07<04:17, 6.33MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  34%|██████▊             | 849M/2.47G [02:09<04:11, 6.45MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  35%|██████▉             | 860M/2.47G [02:11<04:25, 6.06MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  35%|███████             | 870M/2.47G [02:13<04:15, 6.27MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  36%|███████▏            | 881M/2.47G [02:14<04:07, 6.42MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  36%|███████▏            | 891M/2.47G [02:16<04:17, 6.13MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  36%|███████▎            | 902M/2.47G [02:17<04:09, 6.30MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  37%|███████▍            | 912M/2.47G [02:19<04:01, 6.45MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  37%|███████▍            | 923M/2.47G [02:21<04:11, 6.17MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  38%|███████▌            | 933M/2.47G [02:23<04:28, 5.73MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  38%|███████▋            | 944M/2.47G [02:25<04:18, 5.91MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  39%|███████▋            | 954M/2.47G [02:26<04:07, 6.12MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  39%|███████▊            | 965M/2.47G [02:28<04:11, 6.00MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  39%|███████▉            | 975M/2.47G [02:30<04:00, 6.22MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  40%|███████▉            | 986M/2.47G [02:31<03:55, 6.32MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  40%|████████            | 996M/2.47G [02:33<04:02, 6.08MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  41%|███████▋           | 1.01G/2.47G [02:35<03:53, 6.28MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  41%|███████▊           | 1.02G/2.47G [02:36<03:46, 6.42MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  42%|███████▉           | 1.03G/2.47G [02:38<04:02, 5.95MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  42%|███████▉           | 1.04G/2.47G [02:40<03:53, 6.15MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  42%|████████           | 1.05G/2.47G [02:41<03:45, 6.32MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  43%|████████▏          | 1.06G/2.47G [02:43<03:49, 6.15MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  43%|████████▏          | 1.07G/2.47G [02:45<03:41, 6.32MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  44%|████████▎          | 1.08G/2.47G [02:46<03:36, 6.44MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  44%|████████▍          | 1.09G/2.47G [02:48<03:48, 6.05MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  45%|████████▍          | 1.10G/2.47G [02:50<03:39, 6.26MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  45%|████████▌          | 1.11G/2.47G [02:51<03:32, 6.40MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  45%|████████▌          | 1.12G/2.47G [02:53<03:37, 6.21MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  46%|████████▋          | 1.13G/2.47G [02:55<03:29, 6.39MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  46%|████████▊          | 1.14G/2.47G [02:56<03:24, 6.49MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  47%|████████▊          | 1.15G/2.47G [02:58<03:28, 6.32MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  47%|████████▉          | 1.16G/2.47G [03:00<03:23, 6.44MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  48%|█████████          | 1.17G/2.47G [03:01<03:18, 6.54MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  48%|█████████          | 1.18G/2.47G [03:03<03:21, 6.38MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  48%|█████████▏         | 1.20G/2.47G [03:04<03:16, 6.50MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  49%|█████████▎         | 1.21G/2.47G [03:06<03:12, 6.59MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  49%|█████████▎         | 1.22G/2.47G [03:07<03:08, 6.65MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  50%|█████████▍         | 1.23G/2.47G [03:09<03:06, 6.67MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  50%|█████████▌         | 1.24G/2.47G [03:11<03:08, 6.55MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  50%|█████████▌         | 1.25G/2.47G [03:12<03:04, 6.62MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  51%|█████████▋         | 1.26G/2.47G [03:14<03:08, 6.44MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  51%|█████████▊         | 1.27G/2.47G [03:16<03:03, 6.54MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  52%|█████████▊         | 1.28G/2.47G [03:17<03:00, 6.62MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  52%|█████████▉         | 1.29G/2.47G [03:19<02:57, 6.67MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  53%|█████████▉         | 1.30G/2.47G [03:20<02:54, 6.72MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  53%|██████████         | 1.31G/2.47G [03:22<02:52, 6.75MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  53%|██████████▏        | 1.32G/2.47G [03:23<02:56, 6.52MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  54%|██████████▏        | 1.33G/2.47G [03:25<02:52, 6.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  54%|██████████▎        | 1.34G/2.47G [03:27<02:49, 6.67MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  55%|██████████▍        | 1.35G/2.47G [03:28<02:47, 6.69MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  55%|██████████▍        | 1.36G/2.47G [03:30<02:45, 6.71MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  56%|██████████▌        | 1.37G/2.47G [03:31<02:42, 6.74MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  56%|██████████▋        | 1.38G/2.47G [03:33<02:40, 6.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  56%|██████████▋        | 1.39G/2.47G [03:34<02:44, 6.53MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  57%|██████████▊        | 1.41G/2.47G [03:36<02:41, 6.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  57%|██████████▉        | 1.42G/2.47G [03:38<02:38, 6.67MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  58%|██████████▉        | 1.43G/2.47G [03:39<02:35, 6.71MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  58%|███████████        | 1.44G/2.47G [03:41<02:33, 6.74MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  59%|███████████        | 1.45G/2.47G [03:42<02:31, 6.75MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  59%|███████████▏       | 1.46G/2.47G [03:44<02:35, 6.53MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  59%|███████████▎       | 1.47G/2.47G [03:45<02:31, 6.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  60%|███████████▎       | 1.48G/2.47G [03:47<02:28, 6.67MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  60%|███████████▍       | 1.49G/2.47G [03:48<02:26, 6.71MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  61%|███████████▌       | 1.50G/2.47G [03:50<02:24, 6.74MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  61%|███████████▌       | 1.51G/2.47G [03:52<02:22, 6.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  62%|███████████▋       | 1.52G/2.47G [03:53<02:20, 6.78MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  62%|███████████▊       | 1.53G/2.47G [03:55<02:23, 6.54MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  62%|███████████▊       | 1.54G/2.47G [03:56<02:20, 6.62MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  63%|███████████▉       | 1.55G/2.47G [03:58<02:18, 6.66MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  63%|████████████       | 1.56G/2.47G [04:00<02:21, 6.42MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  64%|████████████       | 1.57G/2.47G [04:01<02:17, 6.54MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  64%|████████████▏      | 1.58G/2.47G [04:03<02:14, 6.60MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  64%|████████████▎      | 1.59G/2.47G [04:05<02:22, 6.16MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  65%|████████████▎      | 1.60G/2.47G [04:06<02:16, 6.34MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  65%|████████████▍      | 1.61G/2.47G [04:08<02:12, 6.45MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  66%|████████████▍      | 1.63G/2.47G [04:10<02:21, 5.99MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  66%|████████████▌      | 1.64G/2.47G [04:11<02:14, 6.21MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  67%|████████████▋      | 1.65G/2.47G [04:13<02:09, 6.36MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  67%|████████████▋      | 1.66G/2.47G [04:15<02:14, 6.04MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  67%|████████████▊      | 1.67G/2.47G [04:16<02:09, 6.22MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  68%|████████████▉      | 1.68G/2.47G [04:18<02:04, 6.37MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  68%|████████████▉      | 1.69G/2.47G [04:20<02:08, 6.11MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  69%|█████████████      | 1.70G/2.47G [04:21<02:02, 6.28MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  69%|█████████████▏     | 1.71G/2.47G [04:23<01:58, 6.43MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  70%|█████████████▏     | 1.72G/2.47G [04:25<02:06, 5.94MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  70%|█████████████▎     | 1.73G/2.47G [04:27<02:00, 6.16MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  70%|█████████████▍     | 1.74G/2.47G [04:28<01:56, 6.25MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  71%|█████████████▍     | 1.75G/2.47G [04:30<02:04, 5.81MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  71%|█████████████▌     | 1.76G/2.47G [04:32<01:57, 6.06MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  72%|█████████████▌     | 1.77G/2.47G [04:34<01:56, 6.02MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  72%|█████████████▋     | 1.78G/2.47G [04:35<01:53, 6.07MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  73%|█████████████▊     | 1.79G/2.47G [04:37<01:48, 6.24MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  73%|█████████████▊     | 1.80G/2.47G [04:39<01:47, 6.23MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  73%|█████████████▉     | 1.81G/2.47G [04:40<01:46, 6.20MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  74%|██████████████     | 1.82G/2.47G [04:42<01:41, 6.35MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  74%|██████████████     | 1.84G/2.47G [04:44<01:41, 6.24MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  75%|██████████████▏    | 1.85G/2.47G [04:45<01:42, 6.10MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  75%|██████████████▎    | 1.86G/2.47G [04:47<01:38, 6.28MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  76%|██████████████▎    | 1.87G/2.47G [04:49<01:37, 6.18MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  76%|██████████████▍    | 1.88G/2.47G [04:51<01:36, 6.13MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  76%|██████████████▌    | 1.89G/2.47G [04:52<01:32, 6.31MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  77%|██████████████▌    | 1.90G/2.47G [04:54<01:33, 6.14MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  77%|██████████████▋    | 1.91G/2.47G [04:56<01:31, 6.15MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  78%|██████████████▊    | 1.92G/2.47G [04:57<01:27, 6.32MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  78%|██████████████▊    | 1.93G/2.47G [04:59<01:28, 6.15MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  78%|██████████████▉    | 1.94G/2.47G [05:01<01:26, 6.16MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  79%|██████████████▉    | 1.95G/2.47G [05:02<01:22, 6.32MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  79%|███████████████    | 1.96G/2.47G [05:04<01:23, 6.15MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  80%|███████████████▏   | 1.97G/2.47G [05:06<01:19, 6.31MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  80%|███████████████▏   | 1.98G/2.47G [05:07<01:16, 6.43MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  81%|███████████████▎   | 1.99G/2.47G [05:09<01:15, 6.34MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  81%|███████████████▍   | 2.00G/2.47G [05:11<01:17, 6.09MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  81%|███████████████▍   | 2.01G/2.47G [05:12<01:12, 6.28MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  82%|███████████████▌   | 2.02G/2.47G [05:14<01:14, 6.03MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  82%|███████████████▋   | 2.03G/2.47G [05:16<01:10, 6.25MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  83%|███████████████▋   | 2.04G/2.47G [05:17<01:06, 6.38MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  83%|███████████████▊   | 2.06G/2.47G [05:19<01:08, 6.06MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  84%|███████████████▉   | 2.07G/2.47G [05:21<01:07, 6.04MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  84%|███████████████▉   | 2.08G/2.47G [05:23<01:03, 6.24MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  84%|████████████████   | 2.09G/2.47G [05:24<01:02, 6.12MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  85%|████████████████   | 2.10G/2.47G [05:26<00:59, 6.29MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  85%|████████████████▏  | 2.11G/2.47G [05:27<00:56, 6.44MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  86%|████████████████▎  | 2.12G/2.47G [05:29<00:56, 6.24MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  86%|████████████████▎  | 2.13G/2.47G [05:31<00:53, 6.39MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  87%|████████████████▍  | 2.14G/2.47G [05:32<00:51, 6.51MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  87%|████████████████▌  | 2.15G/2.47G [05:34<00:53, 6.06MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  87%|████████████████▌  | 2.16G/2.47G [05:36<00:49, 6.26MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  88%|████████████████▋  | 2.17G/2.47G [05:37<00:46, 6.41MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  88%|████████████████▊  | 2.18G/2.47G [05:39<00:46, 6.25MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  89%|████████████████▊  | 2.19G/2.47G [05:41<00:43, 6.41MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  89%|████████████████▉  | 2.20G/2.47G [05:42<00:41, 6.52MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  90%|█████████████████  | 2.21G/2.47G [05:44<00:39, 6.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  90%|█████████████████  | 2.22G/2.47G [05:46<00:38, 6.43MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  90%|█████████████████▏ | 2.23G/2.47G [05:47<00:36, 6.54MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  91%|█████████████████▏ | 2.24G/2.47G [05:49<00:34, 6.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  91%|█████████████████▎ | 2.25G/2.47G [05:50<00:32, 6.67MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  92%|█████████████████▍ | 2.26G/2.47G [05:52<00:30, 6.71MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  92%|█████████████████▍ | 2.28G/2.47G [05:53<00:29, 6.74MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  92%|█████████████████▌ | 2.29G/2.47G [05:55<00:28, 6.52MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  93%|█████████████████▋ | 2.30G/2.47G [05:57<00:26, 6.60MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  93%|█████████████████▋ | 2.31G/2.47G [05:58<00:24, 6.66MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  94%|█████████████████▊ | 2.32G/2.47G [06:00<00:23, 6.71MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  94%|█████████████████▉ | 2.33G/2.47G [06:01<00:21, 6.74MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  95%|█████████████████▉ | 2.34G/2.47G [06:03<00:19, 6.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  95%|██████████████████ | 2.35G/2.47G [06:04<00:18, 6.62MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  95%|██████████████████▏| 2.36G/2.47G [06:06<00:17, 6.41MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  96%|██████████████████▏| 2.37G/2.47G [06:08<00:15, 6.52MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  96%|██████████████████▎| 2.38G/2.47G [06:09<00:13, 6.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  97%|██████████████████▍| 2.39G/2.47G [06:11<00:12, 6.66MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  97%|██████████████████▍| 2.40G/2.47G [06:12<00:10, 6.71MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  98%|██████████████████▌| 2.41G/2.47G [06:14<00:08, 6.73MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  98%|██████████████████▌| 2.42G/2.47G [06:15<00:07, 6.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  98%|██████████████████▋| 2.43G/2.47G [06:17<00:05, 6.53MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  99%|██████████████████▊| 2.44G/2.47G [06:19<00:04, 6.59MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  99%|██████████████████▊| 2.45G/2.47G [06:20<00:02, 6.56MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors: 100%|██████████████████▉| 2.46G/2.47G [06:22<00:01, 6.63MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors: 100%|███████████████████| 2.47G/2.47G [06:23<00:00, 6.45MB/s]\u001b[A\u001b[A\n",
      "Fetching 5 files: 100%|███████████████████████████| 5/5 [06:24<00:00, 76.89s/it]\n",
      "Converting .safetensor files to PyTorch binaries (.bin)\n",
      "checkpoints/meta-llama/Llama-3.2-1B-Instruct/model.safetensors --> checkpoints/meta-llama/Llama-3.2-1B-Instruct/model.bin\n",
      "Converting checkpoint files to LitGPT format.\n",
      "{'checkpoint_dir': PosixPath('checkpoints/meta-llama/Llama-3.2-1B-Instruct'),\n",
      " 'debug_mode': False,\n",
      " 'dtype': None,\n",
      " 'model_name': None}\n",
      "Loading weights: model.bin: 100%|███████████████████████| 00:02<00:00, 42.84it/s\n",
      "Saving converted checkpoint to checkpoints/meta-llama/Llama-3.2-1B-Instruct\n"
     ]
    }
   ],
   "source": [
    "!litgpt download meta-llama/Llama-3.2-1B-Instruct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Download Dataset\n",
    "We will download finance_alpaca data and take only 100 records for the fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1186  100  1186    0     0   1691      0 --:--:-- --:--:-- --:--:--  1689\n",
      "100 21.1M  100 21.1M    0     0  4752k      0  0:00:04  0:00:04 --:--:-- 6547k 0     0  4868k      0  0:00:04  0:00:04 --:--:-- 6555k\n"
     ]
    }
   ],
   "source": [
    "!curl -L https://huggingface.co/datasets/ksaw008/finance_alpaca/resolve/main/finance_alpaca.json -o my_custom_dataset.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need 100 samples for fine tuning to test how litGPT finetune work. As a result, we take first 100 records of finance_alpaca dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('my_custom_dataset.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "sample_data = data[0:100]\n",
    "\n",
    "\n",
    "with open('my_custom_dataset_small.json', 'w') as f:\n",
    "    json.dump(sample_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the begin and end of the sample data to ensure it is in `JSON` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"instruction\": \"How do dividend policies impact a company's financial performance?\",\n",
      "        \"input\": \"\",\n",
      "        \"output\": \"Dividend policies of a company can significantly impact its financial performance in several ways. Here are the steps outlining how this happens:\\n\\n1. Retained Earnings: When a company pays dividends, it reduces the amount of retained earnings it has. Retained earnings are a source of internal finance that a company can use to reinvest in its business or pay off its liabilities. Therefore, a high dividend payout can limit a company's financial flexibility and growth potential.\\n\\n2. Signal to Investors: Dividend policies can also send signals to the market about a company's future prospects. A stable or increasing dividend payout can be seen as a sign of a company's strong financial health and future profitability, which can boost investor confidence and potentially increase the company's share price. Conversely, a reduction or omission of dividends can be interpreted as a sign of financial distress, which can negatively impact the company's market valuation.\\n\\n3. Cost of Capital: Dividend policies can influence a company's cost of capital. High dividend payouts can increase the cost of equity as shareholders may demand a higher rate of return to compensate for the reduced capital gains. This can increase the company's weighted average cost of capital (WACC) and decrease the net present value (NPV) of its future cash flows, making its investment projects less attractive.\\n\\n4. Tax Considerations: In many jurisdictions, dividends are taxed at a higher rate than capital gains. Therefore, a high dividend payout policy can increase the tax burden for shareholders, which can reduce their net return and potentially make the company's shares less attractive.\\n\\n5. Liquidity and Solvency: Regular dividend payouts require a company to have sufficient liquidity. If a company has a high dividend payout but faces cash flow issues, it may need to borrow funds to meet its dividend obligations, which can increase its financial risk and potentially affect its solvency.\\n\\n6. Agency Costs: Dividend policies can also impact agency costs. If a company retains more earnings, managers may have more funds at their disposal, which can potentially lead to overinvestment or wasteful spending. By paying out dividends, a company can reduce these agency costs, as it forces managers to be more disciplined and efficient in their capital allocation decisions.\\n\\nIn conclusion, while dividends can provide a steady income stream for shareholders, their impact on a company's financial performance needs to be carefully considered. A balance must be struck between rewarding shareholders and ensuring the company has sufficient funds to invest in profitable opportunities and maintain its financial health.\"\n",
      "    },\n",
      "    {\n",
      "        \"instruction\": \"What are the potential challenges in forecasting interest expenses for financial statement analysis?\",\n",
      "        \"input\": \"\",\n",
      "        \"output\": \"Forecasting interest expenses for financial statement analysis involves several potential challenges. Here are some of them:\\n\\n1. Interest Rate Volatility: One of the most significant challenges in forecasting interest expenses is predicting future interest rates. Interest rates can be highly volatile and are influenced by various factors such as inflation, economic growth, supply and demand for credit, and monetary policy. If interest rates increase, the interest expense for a company with variable-rate debt will also increase. This uncertainty makes it difficult to accurately forecast interest expenses.\\n\\n2. Changes in Debt Levels: The level of a company's debt can change over time due to new borrowings, repayments, or refinancing. These changes can have a substantial impact on a company's interest expense, making it difficult to forecast accurately.\\n\\n3. Refinancing Risk: Companies often refinance their debt to take advantage of lower interest rates or better loan terms. However, the ability to refinance and the terms of the new debt can be uncertain, making it challenging to forecast interest expenses accurately.\\n\\n4. Changes in Credit Rating: If a company's credit rating changes, it can significantly affect the interest rate it pays on its debt. A downgrade in credit rating usually results in higher interest rates, while an upgrade can lower them. Predicting changes in credit ratings can be challenging, adding to the difficulty of forecasting interest expenses.\\n\\n5. Changes in Capital Structure: Changes in a company's capital structure, such as a shift from equity to debt or vice versa, can also affect its interest expense. For instance, if a company decides to issue more debt, its interest expense will increase, assuming all other factors remain constant.\\n\\n6. Tax Laws: Changes in tax laws can impact the after-tax cost of debt, which in turn affects the interest expense. For instance, if the tax deductibility of interest expense is reduced or eliminated, the after-tax cost of debt will increase.\\n\\n7. Foreign Exchange Rates: For companies with debt denominated in foreign currencies, changes in exchange rates can significantly impact the interest expense when translated back into the company's reporting currency.\\n\\n8. Accounting Changes: Changes in accounting standards or practices can also affect the reported interest expense. For example, changes in the way leases are accounted for can impact the amount of interest expense reported.\\n\\nAll these factors make the task of forecasting interest expenses quite complex and uncertain. Therefore, analysts need to consider all these factors and make appropriate assumptions when forecasting interest expenses.\"\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 my_custom_dataset_small.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \"instruction\": \"How does operating profit margin indicate a company's operational efficiency and profitability?\",\n",
      "        \"input\": \"\",\n",
      "        \"output\": \"Operating profit margin is a financial metric that indicates a company's operational efficiency and profitability by measuring the percentage of revenue that remains after deducting operating expenses. It is calculated by dividing operating profit by revenue and multiplying the result by 100.\\n\\nTo understand how operating profit margin indicates a company's operational efficiency and profitability, we can follow these steps:\\n\\nStep 1: Understand the components of operating profit: Operating profit is the profit a company generates from its core operations before interest and taxes. It includes revenue from the company's primary activities and subtracts all operating expenses, such as cost of goods sold, selling and administrative expenses, and research and development costs.\\n\\nStep 2: Calculate operating profit margin: To calculate the operating profit margin, divide the operating profit by revenue and multiply the result by 100. The formula is as follows:\\n\\nOperating Profit Margin = (Operating Profit / Revenue) * 100\\n\\nStep 3: Interpret the operating profit margin: The resulting percentage represents the proportion of revenue that the company retains as operating profit after deducting all operating expenses. A higher operating profit margin indicates greater operational efficiency and profitability, while a lower margin suggests lower efficiency and profitability.\\n\\nStep 4: Analyze the trend over time: It is essential to analyze the operating profit margin over multiple periods to identify any trends. If the margin is consistently increasing, it suggests that the company is improving its operational efficiency and profitability. Conversely, a declining margin may indicate deteriorating operational performance.\\n\\nStep 5: Compare with industry peers: To assess a company's operational efficiency and profitability, it is crucial to compare its operating profit margin with industry peers. If the company's margin is higher than its competitors, it indicates that it is more efficient and profitable in its operations. Conversely, a lower margin suggests the company may be facing challenges or is less efficient compared to its peers.\\n\\nStep 6: Consider other factors: While operating profit margin is a useful metric, it should be evaluated in conjunction with other financial ratios and factors. Factors like industry dynamics, market conditions, and company-specific factors should be considered to gain a comprehensive understanding of a company's operational efficiency and profitability.\\n\\nOverall, operating profit margin provides insights into a company's operational efficiency and profitability by measuring how effectively it generates profits from its core operations. By analyzing this metric over time and comparing it with industry peers, investors and analysts can make informed decisions about a company's financial health and prospects.\"\n",
      "    },\n",
      "    {\n",
      "        \"instruction\": \"How is alpha calculated and interpreted in portfolio evaluation?\",\n",
      "        \"input\": \"\",\n",
      "        \"output\": \"Step 1: Understand the concept of alpha\\nAlpha is a measure of a portfolio's performance relative to a benchmark index. It is used to evaluate the ability of a portfolio manager to generate excess returns, which are returns that exceed the benchmark's returns. A positive alpha indicates that the portfolio has outperformed the benchmark, while a negative alpha indicates underperformance.\\n\\nStep 2: Gather the necessary data\\nTo calculate alpha, you will need the following data:\\n- Monthly or quarterly returns of the portfolio\\n- Monthly or quarterly returns of the benchmark index\\n- Risk-free rate of return (typically the yield on government bonds)\\n\\nStep 3: Calculate the excess returns\\nFirst, calculate the excess returns of the portfolio by subtracting the risk-free rate from the portfolio returns for each period. This adjustment accounts for the opportunity cost of not investing in risk-free assets.\\n\\nExcess Return = Portfolio Return - Risk-Free Rate\\n\\nStep 4: Calculate the excess returns of the benchmark\\nSimilarly, calculate the excess returns of the benchmark by subtracting the risk-free rate from the benchmark returns for each period.\\n\\nExcess Return (Benchmark) = Benchmark Return - Risk-Free Rate\\n\\nStep 5: Calculate the average excess returns\\nCalculate the average excess returns for both the portfolio and the benchmark by taking the arithmetic mean of the excess returns calculated in steps 3 and 4.\\n\\nAverage Excess Return (Portfolio) = Sum of Excess Returns (Portfolio) / Number of Periods\\nAverage Excess Return (Benchmark) = Sum of Excess Returns (Benchmark) / Number of Periods\\n\\nStep 6: Calculate the beta\\nBeta measures the sensitivity of the portfolio's returns to changes in the benchmark's returns. It is calculated using regression analysis, which compares the historical returns of the portfolio to the benchmark. The slope of the regression line represents the beta.\\n\\nStep 7: Calculate the alpha\\nFinally, calculate the alpha by subtracting the product of the beta and the average excess return of the benchmark from the average excess return of the portfolio.\\n\\nAlpha = Average Excess Return (Portfolio) - (Beta * Average Excess Return (Benchmark))\\n\\nStep 8: Interpret the alpha\\nA positive alpha indicates that the portfolio has outperformed the benchmark after adjusting for risk. It suggests that the portfolio manager has added value through superior stock selection or market timing. Conversely, a negative alpha suggests that the portfolio has underperformed the benchmark, indicating poor stock selection or market timing.\\n\\nIt is important to note that alpha is not the only measure of portfolio performance and should be considered alongside other performance metrics such as beta, standard deviation, and Sharpe ratio to gain a comprehensive understanding of the portfolio's risk-adjusted returns.\"\n",
      "    }\n",
      "]"
     ]
    }
   ],
   "source": [
    "!tail -n 10 my_custom_dataset_small.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Finetune the model on a custom dataset\n",
    "\n",
    "Fine-tuning a large language model involves taking a pre-trained model and training it further on a specialized dataset. This process adjusts the model's parameters to better handle tasks or domains not covered by the original training, improving its performance in more specific or niche applications.\n",
    "\n",
    "LitGPT currently supports the following finetuning methods:\n",
    "\n",
    "* `litgpt finetune_full`\n",
    "\n",
    "* `litgpt finetune_lora`\n",
    "\n",
    "* `litgpt finetune_adapter`\n",
    "\n",
    "* `litgpt finetune_adapter_v2`\n",
    "\n",
    "We only going to use `litgpt finetune_lora`. What could not confirm is that if `litgpt finetune` is the same as `litgpt finetune_lora`.\n",
    "\n",
    "- LoRA uses low-rank adapters to fine-tune only a small set of new parameters, keeping the main model weights frozen. This approach greatly reduces memory usage compared to full fine-tuning.\n",
    "\n",
    "- QLoRA goes a step further by quantizing the model weights (often to 4-bit precision) before applying LoRA. This combination lowers memory costs even more while still allowing effective fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'access_token': None,\n",
      " 'checkpoint_dir': PosixPath('checkpoints/meta-llama/Llama-3.2-1B-Instruct'),\n",
      " 'data': JSON(json_path=PosixPath('my_custom_dataset_small.json'),\n",
      "              mask_prompt=False,\n",
      "              val_split_fraction=0.1,\n",
      "              prompt_style=<litgpt.prompts.Alpaca object at 0x33ad2e9c0>,\n",
      "              ignore_index=-100,\n",
      "              seed=42,\n",
      "              num_workers=4),\n",
      " 'devices': 1,\n",
      " 'eval': EvalArgs(interval=100,\n",
      "                  max_new_tokens=100,\n",
      "                  max_iters=100,\n",
      "                  initial_validation=False,\n",
      "                  final_validation=True,\n",
      "                  evaluate_example='first'),\n",
      " 'logger_name': 'csv',\n",
      " 'lora_alpha': 16,\n",
      " 'lora_dropout': 0.05,\n",
      " 'lora_head': False,\n",
      " 'lora_key': False,\n",
      " 'lora_mlp': False,\n",
      " 'lora_projection': False,\n",
      " 'lora_query': True,\n",
      " 'lora_r': 8,\n",
      " 'lora_value': True,\n",
      " 'num_nodes': 1,\n",
      " 'optimizer': 'AdamW',\n",
      " 'out_dir': PosixPath('out/llama-custom-model'),\n",
      " 'precision': None,\n",
      " 'quantize': None,\n",
      " 'seed': 1337,\n",
      " 'train': TrainArgs(save_interval=1000,\n",
      "                    log_interval=1,\n",
      "                    global_batch_size=16,\n",
      "                    micro_batch_size=1,\n",
      "                    lr_warmup_steps=100,\n",
      "                    lr_warmup_fraction=None,\n",
      "                    epochs=5,\n",
      "                    max_tokens=None,\n",
      "                    max_steps=None,\n",
      "                    max_seq_length=None,\n",
      "                    tie_embeddings=None,\n",
      "                    max_norm=None,\n",
      "                    min_lr=6e-05)}\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "Seed set to 1337\n",
      "Number of trainable parameters: 851,968\n",
      "Number of non-trainable parameters: 1,498,482,688\n",
      "The longest sequence length in the train data is 794, the model's maximum sequence length is 794 and context length is 131072\n",
      "Verifying settings ...\n",
      "/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/torch/amp/autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Epoch 1 | iter 1 step 0 | loss train: 0.980, val: n/a | iter time: 9358.55 ms\n",
      "Epoch 1 | iter 2 step 0 | loss train: 1.070, val: n/a | iter time: 419.92 ms\n",
      "Epoch 1 | iter 3 step 0 | loss train: 1.147, val: n/a | iter time: 422.94 ms\n",
      "Epoch 1 | iter 4 step 0 | loss train: 1.198, val: n/a | iter time: 537.42 ms\n",
      "Epoch 1 | iter 5 step 0 | loss train: 1.208, val: n/a | iter time: 471.76 ms\n",
      "Epoch 1 | iter 6 step 0 | loss train: 1.232, val: n/a | iter time: 540.72 ms\n",
      "Epoch 1 | iter 7 step 0 | loss train: 1.232, val: n/a | iter time: 385.41 ms\n",
      "Epoch 1 | iter 8 step 0 | loss train: 1.215, val: n/a | iter time: 471.51 ms\n",
      "Epoch 1 | iter 9 step 0 | loss train: 1.218, val: n/a | iter time: 469.06 ms\n",
      "Epoch 1 | iter 10 step 0 | loss train: 1.180, val: n/a | iter time: 658.46 ms\n",
      "Epoch 1 | iter 11 step 0 | loss train: 1.194, val: n/a | iter time: 559.17 ms\n",
      "Epoch 1 | iter 12 step 0 | loss train: 1.191, val: n/a | iter time: 593.24 ms\n",
      "Epoch 1 | iter 13 step 0 | loss train: 1.184, val: n/a | iter time: 627.97 ms\n",
      "Epoch 1 | iter 14 step 0 | loss train: 1.192, val: n/a | iter time: 571.20 ms\n",
      "Epoch 1 | iter 15 step 0 | loss train: 1.175, val: n/a | iter time: 576.37 ms\n",
      "Epoch 1 | iter 16 step 1 | loss train: 1.181, val: n/a | iter time: 443.16 ms (step)\n",
      "Epoch 1 | iter 17 step 1 | loss train: 1.204, val: n/a | iter time: 427.88 ms\n",
      "Epoch 1 | iter 18 step 1 | loss train: 1.212, val: n/a | iter time: 489.37 ms\n",
      "Epoch 1 | iter 19 step 1 | loss train: 1.201, val: n/a | iter time: 560.73 ms\n",
      "Epoch 1 | iter 20 step 1 | loss train: 1.176, val: n/a | iter time: 712.66 ms\n",
      "Epoch 1 | iter 21 step 1 | loss train: 1.176, val: n/a | iter time: 534.37 ms\n",
      "Epoch 1 | iter 22 step 1 | loss train: 1.181, val: n/a | iter time: 328.52 ms\n",
      "Epoch 1 | iter 23 step 1 | loss train: 1.164, val: n/a | iter time: 703.88 ms\n",
      "Epoch 1 | iter 24 step 1 | loss train: 1.188, val: n/a | iter time: 457.97 ms\n",
      "Epoch 1 | iter 25 step 1 | loss train: 1.224, val: n/a | iter time: 551.78 ms\n",
      "Epoch 1 | iter 26 step 1 | loss train: 1.243, val: n/a | iter time: 391.35 ms\n",
      "Epoch 1 | iter 27 step 1 | loss train: 1.230, val: n/a | iter time: 453.72 ms\n",
      "Epoch 1 | iter 28 step 1 | loss train: 1.236, val: n/a | iter time: 449.27 ms\n",
      "Epoch 1 | iter 29 step 1 | loss train: 1.260, val: n/a | iter time: 553.22 ms\n",
      "Epoch 1 | iter 30 step 1 | loss train: 1.267, val: n/a | iter time: 557.25 ms\n",
      "Epoch 1 | iter 31 step 1 | loss train: 1.277, val: n/a | iter time: 468.52 ms\n",
      "Epoch 1 | iter 32 step 2 | loss train: 1.273, val: n/a | iter time: 529.05 ms (step)\n",
      "Epoch 1 | iter 33 step 2 | loss train: 1.274, val: n/a | iter time: 452.61 ms\n",
      "Epoch 1 | iter 34 step 2 | loss train: 1.270, val: n/a | iter time: 516.23 ms\n",
      "Epoch 1 | iter 35 step 2 | loss train: 1.283, val: n/a | iter time: 544.00 ms\n",
      "Epoch 1 | iter 36 step 2 | loss train: 1.322, val: n/a | iter time: 422.39 ms\n",
      "Epoch 1 | iter 37 step 2 | loss train: 1.313, val: n/a | iter time: 608.99 ms\n",
      "Epoch 1 | iter 38 step 2 | loss train: 1.305, val: n/a | iter time: 477.50 ms\n",
      "Epoch 1 | iter 39 step 2 | loss train: 1.311, val: n/a | iter time: 369.04 ms\n",
      "Epoch 1 | iter 40 step 2 | loss train: 1.284, val: n/a | iter time: 458.01 ms\n",
      "Epoch 1 | iter 41 step 2 | loss train: 1.264, val: n/a | iter time: 500.56 ms\n",
      "Epoch 1 | iter 42 step 2 | loss train: 1.268, val: n/a | iter time: 374.59 ms\n",
      "Epoch 1 | iter 43 step 2 | loss train: 1.278, val: n/a | iter time: 505.36 ms\n",
      "Epoch 1 | iter 44 step 2 | loss train: 1.283, val: n/a | iter time: 406.71 ms\n",
      "Epoch 1 | iter 45 step 2 | loss train: 1.269, val: n/a | iter time: 523.94 ms\n",
      "Epoch 1 | iter 46 step 2 | loss train: 1.251, val: n/a | iter time: 417.94 ms\n",
      "Epoch 1 | iter 47 step 2 | loss train: 1.274, val: n/a | iter time: 447.77 ms\n",
      "Epoch 1 | iter 48 step 3 | loss train: 1.278, val: n/a | iter time: 521.00 ms (step)\n",
      "Epoch 1 | iter 49 step 3 | loss train: 1.254, val: n/a | iter time: 423.19 ms\n",
      "Epoch 1 | iter 50 step 3 | loss train: 1.243, val: n/a | iter time: 521.88 ms\n",
      "Epoch 1 | iter 51 step 3 | loss train: 1.223, val: n/a | iter time: 434.01 ms\n",
      "Epoch 1 | iter 52 step 3 | loss train: 1.190, val: n/a | iter time: 518.37 ms\n",
      "Epoch 1 | iter 53 step 3 | loss train: 1.217, val: n/a | iter time: 425.92 ms\n",
      "Epoch 1 | iter 54 step 3 | loss train: 1.228, val: n/a | iter time: 545.00 ms\n",
      "Epoch 1 | iter 55 step 3 | loss train: 1.223, val: n/a | iter time: 615.38 ms\n",
      "Epoch 1 | iter 56 step 3 | loss train: 1.232, val: n/a | iter time: 471.29 ms\n",
      "Epoch 1 | iter 57 step 3 | loss train: 1.220, val: n/a | iter time: 447.17 ms\n",
      "Epoch 1 | iter 58 step 3 | loss train: 1.214, val: n/a | iter time: 509.02 ms\n",
      "Epoch 1 | iter 59 step 3 | loss train: 1.241, val: n/a | iter time: 424.60 ms\n",
      "Epoch 1 | iter 60 step 3 | loss train: 1.214, val: n/a | iter time: 578.25 ms\n",
      "Epoch 1 | iter 61 step 3 | loss train: 1.221, val: n/a | iter time: 481.94 ms\n",
      "Epoch 1 | iter 62 step 3 | loss train: 1.232, val: n/a | iter time: 336.12 ms\n",
      "Epoch 1 | iter 63 step 3 | loss train: 1.231, val: n/a | iter time: 440.58 ms\n",
      "Epoch 1 | iter 64 step 4 | loss train: 1.223, val: n/a | iter time: 483.38 ms (step)\n",
      "Epoch 1 | iter 65 step 4 | loss train: 1.231, val: n/a | iter time: 533.31 ms\n",
      "Epoch 1 | iter 66 step 4 | loss train: 1.244, val: n/a | iter time: 489.37 ms\n",
      "Epoch 1 | iter 67 step 4 | loss train: 1.256, val: n/a | iter time: 774.11 ms\n",
      "Epoch 1 | iter 68 step 4 | loss train: 1.263, val: n/a | iter time: 457.98 ms\n",
      "Epoch 1 | iter 69 step 4 | loss train: 1.256, val: n/a | iter time: 669.56 ms\n",
      "Epoch 1 | iter 70 step 4 | loss train: 1.239, val: n/a | iter time: 469.00 ms\n",
      "Epoch 1 | iter 71 step 4 | loss train: 1.248, val: n/a | iter time: 708.61 ms\n",
      "Epoch 1 | iter 72 step 4 | loss train: 1.255, val: n/a | iter time: 442.17 ms\n",
      "Epoch 1 | iter 73 step 4 | loss train: 1.260, val: n/a | iter time: 495.14 ms\n",
      "Epoch 1 | iter 74 step 4 | loss train: 1.254, val: n/a | iter time: 544.38 ms\n",
      "Epoch 1 | iter 75 step 4 | loss train: 1.234, val: n/a | iter time: 498.37 ms\n",
      "Epoch 1 | iter 76 step 4 | loss train: 1.248, val: n/a | iter time: 698.87 ms\n",
      "Epoch 1 | iter 77 step 4 | loss train: 1.242, val: n/a | iter time: 601.03 ms\n",
      "Epoch 1 | iter 78 step 4 | loss train: 1.236, val: n/a | iter time: 517.90 ms\n",
      "Epoch 1 | iter 79 step 4 | loss train: 1.235, val: n/a | iter time: 590.08 ms\n",
      "Epoch 1 | iter 80 step 5 | loss train: 1.254, val: n/a | iter time: 557.11 ms (step)\n",
      "Epoch 1 | iter 81 step 5 | loss train: 1.284, val: n/a | iter time: 547.92 ms\n",
      "Epoch 1 | iter 82 step 5 | loss train: 1.295, val: n/a | iter time: 501.59 ms\n",
      "Epoch 1 | iter 83 step 5 | loss train: 1.315, val: n/a | iter time: 545.83 ms\n",
      "Epoch 1 | iter 84 step 5 | loss train: 1.326, val: n/a | iter time: 450.69 ms\n",
      "Epoch 1 | iter 85 step 5 | loss train: 1.335, val: n/a | iter time: 490.82 ms\n",
      "Epoch 1 | iter 86 step 5 | loss train: 1.351, val: n/a | iter time: 388.13 ms\n",
      "Epoch 1 | iter 87 step 5 | loss train: 1.375, val: n/a | iter time: 457.72 ms\n",
      "Epoch 1 | iter 88 step 5 | loss train: 1.365, val: n/a | iter time: 521.89 ms\n",
      "Epoch 1 | iter 89 step 5 | loss train: 1.363, val: n/a | iter time: 474.31 ms\n",
      "Epoch 1 | iter 90 step 5 | loss train: 1.375, val: n/a | iter time: 514.23 ms\n",
      "Epoch 2 | iter 91 step 5 | loss train: 1.360, val: n/a | iter time: 29382.05 ms\n",
      "Epoch 2 | iter 92 step 5 | loss train: 1.367, val: n/a | iter time: 441.44 ms\n",
      "Epoch 2 | iter 93 step 5 | loss train: 1.384, val: n/a | iter time: 342.35 ms\n",
      "Epoch 2 | iter 94 step 5 | loss train: 1.371, val: n/a | iter time: 518.21 ms\n",
      "Epoch 2 | iter 95 step 5 | loss train: 1.350, val: n/a | iter time: 391.20 ms\n",
      "Epoch 2 | iter 96 step 6 | loss train: 1.345, val: n/a | iter time: 388.93 ms (step)\n",
      "Epoch 2 | iter 97 step 6 | loss train: 1.305, val: n/a | iter time: 633.47 ms\n",
      "Epoch 2 | iter 98 step 6 | loss train: 1.306, val: n/a | iter time: 342.77 ms\n",
      "Epoch 2 | iter 99 step 6 | loss train: 1.280, val: n/a | iter time: 380.69 ms\n",
      "Epoch 2 | iter 100 step 6 | loss train: 1.286, val: n/a | iter time: 434.61 ms\n",
      "Epoch 2 | iter 101 step 6 | loss train: 1.251, val: n/a | iter time: 431.47 ms\n",
      "Epoch 2 | iter 102 step 6 | loss train: 1.267, val: n/a | iter time: 299.36 ms\n",
      "Epoch 2 | iter 103 step 6 | loss train: 1.247, val: n/a | iter time: 389.03 ms\n",
      "Epoch 2 | iter 104 step 6 | loss train: 1.251, val: n/a | iter time: 386.59 ms\n",
      "Epoch 2 | iter 105 step 6 | loss train: 1.249, val: n/a | iter time: 469.78 ms\n",
      "Epoch 2 | iter 106 step 6 | loss train: 1.255, val: n/a | iter time: 438.21 ms\n",
      "Epoch 2 | iter 107 step 6 | loss train: 1.268, val: n/a | iter time: 382.76 ms\n",
      "Epoch 2 | iter 108 step 6 | loss train: 1.256, val: n/a | iter time: 298.58 ms\n",
      "Epoch 2 | iter 109 step 6 | loss train: 1.252, val: n/a | iter time: 338.76 ms\n",
      "Epoch 2 | iter 110 step 6 | loss train: 1.290, val: n/a | iter time: 339.14 ms\n",
      "Epoch 2 | iter 111 step 6 | loss train: 1.298, val: n/a | iter time: 434.71 ms\n",
      "Epoch 2 | iter 112 step 7 | loss train: 1.288, val: n/a | iter time: 438.67 ms (step)\n",
      "Epoch 2 | iter 113 step 7 | loss train: 1.321, val: n/a | iter time: 385.69 ms\n",
      "Epoch 2 | iter 114 step 7 | loss train: 1.322, val: n/a | iter time: 371.42 ms\n",
      "Epoch 2 | iter 115 step 7 | loss train: 1.330, val: n/a | iter time: 382.59 ms\n",
      "Epoch 2 | iter 116 step 7 | loss train: 1.319, val: n/a | iter time: 379.03 ms\n",
      "Epoch 2 | iter 117 step 7 | loss train: 1.349, val: n/a | iter time: 357.16 ms\n",
      "Epoch 2 | iter 118 step 7 | loss train: 1.320, val: n/a | iter time: 339.83 ms\n",
      "Epoch 2 | iter 119 step 7 | loss train: 1.335, val: n/a | iter time: 440.26 ms\n",
      "Epoch 2 | iter 120 step 7 | loss train: 1.321, val: n/a | iter time: 479.24 ms\n",
      "Epoch 2 | iter 121 step 7 | loss train: 1.320, val: n/a | iter time: 379.42 ms\n",
      "Epoch 2 | iter 122 step 7 | loss train: 1.300, val: n/a | iter time: 431.10 ms\n",
      "Epoch 2 | iter 123 step 7 | loss train: 1.299, val: n/a | iter time: 296.68 ms\n",
      "Epoch 2 | iter 124 step 7 | loss train: 1.323, val: n/a | iter time: 436.64 ms\n",
      "Epoch 2 | iter 125 step 7 | loss train: 1.312, val: n/a | iter time: 436.89 ms\n",
      "Epoch 2 | iter 126 step 7 | loss train: 1.283, val: n/a | iter time: 516.42 ms\n",
      "Epoch 2 | iter 127 step 7 | loss train: 1.274, val: n/a | iter time: 456.20 ms\n",
      "Epoch 2 | iter 128 step 8 | loss train: 1.278, val: n/a | iter time: 445.59 ms (step)\n",
      "Epoch 2 | iter 129 step 8 | loss train: 1.249, val: n/a | iter time: 321.95 ms\n",
      "Epoch 2 | iter 130 step 8 | loss train: 1.226, val: n/a | iter time: 433.46 ms\n",
      "Epoch 2 | iter 131 step 8 | loss train: 1.220, val: n/a | iter time: 345.43 ms\n",
      "Epoch 2 | iter 132 step 8 | loss train: 1.198, val: n/a | iter time: 495.36 ms\n",
      "Epoch 2 | iter 133 step 8 | loss train: 1.184, val: n/a | iter time: 489.27 ms\n",
      "Epoch 2 | iter 134 step 8 | loss train: 1.158, val: n/a | iter time: 556.63 ms\n",
      "Epoch 2 | iter 135 step 8 | loss train: 1.156, val: n/a | iter time: 413.21 ms\n",
      "Epoch 2 | iter 136 step 8 | loss train: 1.167, val: n/a | iter time: 341.96 ms\n",
      "Epoch 2 | iter 137 step 8 | loss train: 1.178, val: n/a | iter time: 501.92 ms\n",
      "Epoch 2 | iter 138 step 8 | loss train: 1.182, val: n/a | iter time: 410.94 ms\n",
      "Epoch 2 | iter 139 step 8 | loss train: 1.196, val: n/a | iter time: 412.56 ms\n",
      "Epoch 2 | iter 140 step 8 | loss train: 1.201, val: n/a | iter time: 457.75 ms\n",
      "Epoch 2 | iter 141 step 8 | loss train: 1.190, val: n/a | iter time: 610.64 ms\n",
      "Epoch 2 | iter 142 step 8 | loss train: 1.205, val: n/a | iter time: 363.55 ms\n",
      "Epoch 2 | iter 143 step 8 | loss train: 1.222, val: n/a | iter time: 406.03 ms\n",
      "Epoch 2 | iter 144 step 9 | loss train: 1.222, val: n/a | iter time: 311.17 ms (step)\n",
      "Epoch 2 | iter 145 step 9 | loss train: 1.252, val: n/a | iter time: 463.45 ms\n",
      "Epoch 2 | iter 146 step 9 | loss train: 1.265, val: n/a | iter time: 364.69 ms\n",
      "Epoch 2 | iter 147 step 9 | loss train: 1.268, val: n/a | iter time: 419.52 ms\n",
      "Epoch 2 | iter 148 step 9 | loss train: 1.292, val: n/a | iter time: 365.22 ms\n",
      "Epoch 2 | iter 149 step 9 | loss train: 1.291, val: n/a | iter time: 474.19 ms\n",
      "Epoch 2 | iter 150 step 9 | loss train: 1.314, val: n/a | iter time: 738.40 ms\n",
      "Epoch 2 | iter 151 step 9 | loss train: 1.296, val: n/a | iter time: 499.10 ms\n",
      "Epoch 2 | iter 152 step 9 | loss train: 1.305, val: n/a | iter time: 419.87 ms\n",
      "Epoch 2 | iter 153 step 9 | loss train: 1.297, val: n/a | iter time: 360.83 ms\n",
      "Epoch 2 | iter 154 step 9 | loss train: 1.319, val: n/a | iter time: 546.51 ms\n",
      "Epoch 2 | iter 155 step 9 | loss train: 1.306, val: n/a | iter time: 420.06 ms\n",
      "Epoch 2 | iter 156 step 9 | loss train: 1.293, val: n/a | iter time: 558.39 ms\n",
      "Epoch 2 | iter 157 step 9 | loss train: 1.295, val: n/a | iter time: 679.50 ms\n",
      "Epoch 2 | iter 158 step 9 | loss train: 1.291, val: n/a | iter time: 446.38 ms\n",
      "Epoch 2 | iter 159 step 9 | loss train: 1.274, val: n/a | iter time: 438.33 ms\n",
      "Epoch 2 | iter 160 step 10 | loss train: 1.271, val: n/a | iter time: 435.59 ms (step)\n",
      "Epoch 2 | iter 161 step 10 | loss train: 1.247, val: n/a | iter time: 371.06 ms\n",
      "Epoch 2 | iter 162 step 10 | loss train: 1.237, val: n/a | iter time: 496.17 ms\n",
      "Epoch 2 | iter 163 step 10 | loss train: 1.240, val: n/a | iter time: 512.99 ms\n",
      "Epoch 2 | iter 164 step 10 | loss train: 1.244, val: n/a | iter time: 564.23 ms\n",
      "Epoch 2 | iter 165 step 10 | loss train: 1.226, val: n/a | iter time: 749.61 ms\n",
      "Epoch 2 | iter 166 step 10 | loss train: 1.215, val: n/a | iter time: 584.61 ms\n",
      "Epoch 2 | iter 167 step 10 | loss train: 1.218, val: n/a | iter time: 613.04 ms\n",
      "Epoch 2 | iter 168 step 10 | loss train: 1.217, val: n/a | iter time: 493.20 ms\n",
      "Epoch 2 | iter 169 step 10 | loss train: 1.204, val: n/a | iter time: 600.16 ms\n",
      "Epoch 2 | iter 170 step 10 | loss train: 1.176, val: n/a | iter time: 615.56 ms\n",
      "Epoch 2 | iter 171 step 10 | loss train: 1.173, val: n/a | iter time: 500.76 ms\n",
      "Epoch 2 | iter 172 step 10 | loss train: 1.185, val: n/a | iter time: 487.44 ms\n",
      "Epoch 2 | iter 173 step 10 | loss train: 1.180, val: n/a | iter time: 522.31 ms\n",
      "Epoch 2 | iter 174 step 10 | loss train: 1.173, val: n/a | iter time: 447.88 ms\n",
      "Epoch 2 | iter 175 step 10 | loss train: 1.217, val: n/a | iter time: 354.61 ms\n",
      "Epoch 2 | iter 176 step 11 | loss train: 1.212, val: n/a | iter time: 635.09 ms (step)\n",
      "Epoch 2 | iter 177 step 11 | loss train: 1.232, val: n/a | iter time: 485.89 ms\n",
      "Epoch 2 | iter 178 step 11 | loss train: 1.253, val: n/a | iter time: 565.21 ms\n",
      "Epoch 2 | iter 179 step 11 | loss train: 1.253, val: n/a | iter time: 502.14 ms\n",
      "Epoch 2 | iter 180 step 11 | loss train: 1.267, val: n/a | iter time: 509.75 ms\n",
      "Epoch 3 | iter 181 step 11 | loss train: 1.304, val: n/a | iter time: 29453.95 ms\n",
      "Epoch 3 | iter 182 step 11 | loss train: 1.305, val: n/a | iter time: 294.39 ms\n",
      "Epoch 3 | iter 183 step 11 | loss train: 1.311, val: n/a | iter time: 374.60 ms\n",
      "Epoch 3 | iter 184 step 11 | loss train: 1.323, val: n/a | iter time: 389.78 ms\n",
      "Epoch 3 | iter 185 step 11 | loss train: 1.313, val: n/a | iter time: 433.62 ms\n",
      "Epoch 3 | iter 186 step 11 | loss train: 1.320, val: n/a | iter time: 439.84 ms\n",
      "Epoch 3 | iter 187 step 11 | loss train: 1.309, val: n/a | iter time: 328.63 ms\n",
      "Epoch 3 | iter 188 step 11 | loss train: 1.286, val: n/a | iter time: 344.98 ms\n",
      "Epoch 3 | iter 189 step 11 | loss train: 1.301, val: n/a | iter time: 333.51 ms\n",
      "Epoch 3 | iter 190 step 11 | loss train: 1.286, val: n/a | iter time: 643.38 ms\n",
      "Epoch 3 | iter 191 step 11 | loss train: 1.266, val: n/a | iter time: 434.07 ms\n",
      "Epoch 3 | iter 192 step 12 | loss train: 1.272, val: n/a | iter time: 397.98 ms (step)\n",
      "Epoch 3 | iter 193 step 12 | loss train: 1.271, val: n/a | iter time: 436.15 ms\n",
      "Epoch 3 | iter 194 step 12 | loss train: 1.240, val: n/a | iter time: 478.37 ms\n",
      "Epoch 3 | iter 195 step 12 | loss train: 1.232, val: n/a | iter time: 564.40 ms\n",
      "Epoch 3 | iter 196 step 12 | loss train: 1.207, val: n/a | iter time: 385.51 ms\n",
      "Epoch 3 | iter 197 step 12 | loss train: 1.179, val: n/a | iter time: 428.22 ms\n",
      "Epoch 3 | iter 198 step 12 | loss train: 1.183, val: n/a | iter time: 499.65 ms\n",
      "Epoch 3 | iter 199 step 12 | loss train: 1.183, val: n/a | iter time: 390.19 ms\n",
      "Epoch 3 | iter 200 step 12 | loss train: 1.186, val: n/a | iter time: 428.60 ms\n",
      "Epoch 3 | iter 201 step 12 | loss train: 1.199, val: n/a | iter time: 371.85 ms\n",
      "Epoch 3 | iter 202 step 12 | loss train: 1.247, val: n/a | iter time: 325.63 ms\n",
      "Epoch 3 | iter 203 step 12 | loss train: 1.260, val: n/a | iter time: 389.00 ms\n",
      "Epoch 3 | iter 204 step 12 | loss train: 1.258, val: n/a | iter time: 395.61 ms\n",
      "Epoch 3 | iter 205 step 12 | loss train: 1.241, val: n/a | iter time: 319.28 ms\n",
      "Epoch 3 | iter 206 step 12 | loss train: 1.274, val: n/a | iter time: 474.11 ms\n",
      "Epoch 3 | iter 207 step 12 | loss train: 1.263, val: n/a | iter time: 439.90 ms\n",
      "Epoch 3 | iter 208 step 13 | loss train: 1.275, val: n/a | iter time: 401.79 ms (step)\n",
      "Epoch 3 | iter 209 step 13 | loss train: 1.262, val: n/a | iter time: 677.65 ms\n",
      "Epoch 3 | iter 210 step 13 | loss train: 1.294, val: n/a | iter time: 377.24 ms\n",
      "Epoch 3 | iter 211 step 13 | loss train: 1.296, val: n/a | iter time: 518.46 ms\n",
      "Epoch 3 | iter 212 step 13 | loss train: 1.291, val: n/a | iter time: 451.47 ms\n",
      "Epoch 3 | iter 213 step 13 | loss train: 1.310, val: n/a | iter time: 399.70 ms\n",
      "Epoch 3 | iter 214 step 13 | loss train: 1.320, val: n/a | iter time: 439.45 ms\n",
      "Epoch 3 | iter 215 step 13 | loss train: 1.304, val: n/a | iter time: 548.64 ms\n",
      "Epoch 3 | iter 216 step 13 | loss train: 1.289, val: n/a | iter time: 405.72 ms\n",
      "Epoch 3 | iter 217 step 13 | loss train: 1.289, val: n/a | iter time: 463.42 ms\n",
      "Epoch 3 | iter 218 step 13 | loss train: 1.274, val: n/a | iter time: 370.37 ms\n",
      "Epoch 3 | iter 219 step 13 | loss train: 1.268, val: n/a | iter time: 360.81 ms\n",
      "Epoch 3 | iter 220 step 13 | loss train: 1.279, val: n/a | iter time: 471.57 ms\n",
      "Epoch 3 | iter 221 step 13 | loss train: 1.290, val: n/a | iter time: 407.81 ms\n",
      "Epoch 3 | iter 222 step 13 | loss train: 1.263, val: n/a | iter time: 418.74 ms\n",
      "Epoch 3 | iter 223 step 13 | loss train: 1.251, val: n/a | iter time: 351.11 ms\n",
      "Epoch 3 | iter 224 step 14 | loss train: 1.271, val: n/a | iter time: 323.82 ms (step)\n",
      "Epoch 3 | iter 225 step 14 | loss train: 1.280, val: n/a | iter time: 348.32 ms\n",
      "Epoch 3 | iter 226 step 14 | loss train: 1.265, val: n/a | iter time: 453.24 ms\n",
      "Epoch 3 | iter 227 step 14 | loss train: 1.264, val: n/a | iter time: 579.44 ms\n",
      "Epoch 3 | iter 228 step 14 | loss train: 1.272, val: n/a | iter time: 472.63 ms\n",
      "Epoch 3 | iter 229 step 14 | loss train: 1.254, val: n/a | iter time: 410.02 ms\n",
      "Epoch 3 | iter 230 step 14 | loss train: 1.255, val: n/a | iter time: 411.92 ms\n",
      "Epoch 3 | iter 231 step 14 | loss train: 1.264, val: n/a | iter time: 451.89 ms\n",
      "Epoch 3 | iter 232 step 14 | loss train: 1.252, val: n/a | iter time: 467.53 ms\n",
      "Epoch 3 | iter 233 step 14 | loss train: 1.262, val: n/a | iter time: 405.23 ms\n",
      "Epoch 3 | iter 234 step 14 | loss train: 1.246, val: n/a | iter time: 506.52 ms\n",
      "Epoch 3 | iter 235 step 14 | loss train: 1.245, val: n/a | iter time: 471.77 ms\n",
      "Epoch 3 | iter 236 step 14 | loss train: 1.223, val: n/a | iter time: 577.89 ms\n",
      "Epoch 3 | iter 237 step 14 | loss train: 1.205, val: n/a | iter time: 519.78 ms\n",
      "Epoch 3 | iter 238 step 14 | loss train: 1.214, val: n/a | iter time: 427.70 ms\n",
      "Epoch 3 | iter 239 step 14 | loss train: 1.237, val: n/a | iter time: 513.87 ms\n",
      "Epoch 3 | iter 240 step 15 | loss train: 1.218, val: n/a | iter time: 524.84 ms (step)\n",
      "Epoch 3 | iter 241 step 15 | loss train: 1.231, val: n/a | iter time: 416.33 ms\n",
      "Epoch 3 | iter 242 step 15 | loss train: 1.230, val: n/a | iter time: 298.77 ms\n",
      "Epoch 3 | iter 243 step 15 | loss train: 1.240, val: n/a | iter time: 510.78 ms\n",
      "Epoch 3 | iter 244 step 15 | loss train: 1.243, val: n/a | iter time: 475.40 ms\n",
      "Epoch 3 | iter 245 step 15 | loss train: 1.252, val: n/a | iter time: 464.57 ms\n",
      "Epoch 3 | iter 246 step 15 | loss train: 1.265, val: n/a | iter time: 461.75 ms\n",
      "Epoch 3 | iter 247 step 15 | loss train: 1.275, val: n/a | iter time: 416.38 ms\n",
      "Epoch 3 | iter 248 step 15 | loss train: 1.258, val: n/a | iter time: 588.05 ms\n",
      "Epoch 3 | iter 249 step 15 | loss train: 1.266, val: n/a | iter time: 370.51 ms\n",
      "Epoch 3 | iter 250 step 15 | loss train: 1.261, val: n/a | iter time: 464.31 ms\n",
      "Epoch 3 | iter 251 step 15 | loss train: 1.254, val: n/a | iter time: 415.09 ms\n",
      "Epoch 3 | iter 252 step 15 | loss train: 1.284, val: n/a | iter time: 357.35 ms\n",
      "Epoch 3 | iter 253 step 15 | loss train: 1.312, val: n/a | iter time: 294.99 ms\n",
      "Epoch 3 | iter 254 step 15 | loss train: 1.321, val: n/a | iter time: 345.90 ms\n",
      "Epoch 3 | iter 255 step 15 | loss train: 1.299, val: n/a | iter time: 611.53 ms\n",
      "Epoch 3 | iter 256 step 16 | loss train: 1.295, val: n/a | iter time: 532.02 ms (step)\n",
      "Epoch 3 | iter 257 step 16 | loss train: 1.288, val: n/a | iter time: 385.67 ms\n",
      "Epoch 3 | iter 258 step 16 | loss train: 1.309, val: n/a | iter time: 450.45 ms\n",
      "Epoch 3 | iter 259 step 16 | loss train: 1.296, val: n/a | iter time: 496.57 ms\n",
      "Epoch 3 | iter 260 step 16 | loss train: 1.304, val: n/a | iter time: 386.67 ms\n",
      "Epoch 3 | iter 261 step 16 | loss train: 1.307, val: n/a | iter time: 339.70 ms\n",
      "Epoch 3 | iter 262 step 16 | loss train: 1.271, val: n/a | iter time: 634.87 ms\n",
      "Epoch 3 | iter 263 step 16 | loss train: 1.267, val: n/a | iter time: 450.70 ms\n",
      "Epoch 3 | iter 264 step 16 | loss train: 1.305, val: n/a | iter time: 443.62 ms\n",
      "Epoch 3 | iter 265 step 16 | loss train: 1.303, val: n/a | iter time: 428.74 ms\n",
      "Epoch 3 | iter 266 step 16 | loss train: 1.297, val: n/a | iter time: 547.62 ms\n",
      "Epoch 3 | iter 267 step 16 | loss train: 1.312, val: n/a | iter time: 376.86 ms\n",
      "Epoch 3 | iter 268 step 16 | loss train: 1.286, val: n/a | iter time: 442.97 ms\n",
      "Epoch 3 | iter 269 step 16 | loss train: 1.284, val: n/a | iter time: 504.43 ms\n",
      "Epoch 3 | iter 270 step 16 | loss train: 1.273, val: n/a | iter time: 395.91 ms\n",
      "Epoch 4 | iter 271 step 16 | loss train: 1.268, val: n/a | iter time: 29317.10 ms\n",
      "Epoch 4 | iter 272 step 17 | loss train: 1.245, val: n/a | iter time: 541.03 ms (step)\n",
      "Epoch 4 | iter 273 step 17 | loss train: 1.229, val: n/a | iter time: 389.52 ms\n",
      "Epoch 4 | iter 274 step 17 | loss train: 1.217, val: n/a | iter time: 396.04 ms\n",
      "Epoch 4 | iter 275 step 17 | loss train: 1.251, val: n/a | iter time: 342.96 ms\n",
      "Epoch 4 | iter 276 step 17 | loss train: 1.246, val: n/a | iter time: 265.48 ms\n",
      "Epoch 4 | iter 277 step 17 | loss train: 1.241, val: n/a | iter time: 377.67 ms\n",
      "Epoch 4 | iter 278 step 17 | loss train: 1.253, val: n/a | iter time: 518.27 ms\n",
      "Epoch 4 | iter 279 step 17 | loss train: 1.268, val: n/a | iter time: 344.51 ms\n",
      "Epoch 4 | iter 280 step 17 | loss train: 1.259, val: n/a | iter time: 436.01 ms\n",
      "Epoch 4 | iter 281 step 17 | loss train: 1.246, val: n/a | iter time: 444.95 ms\n",
      "Epoch 4 | iter 282 step 17 | loss train: 1.258, val: n/a | iter time: 497.01 ms\n",
      "Epoch 4 | iter 283 step 17 | loss train: 1.287, val: n/a | iter time: 320.26 ms\n",
      "Epoch 4 | iter 284 step 17 | loss train: 1.313, val: n/a | iter time: 438.45 ms\n",
      "Epoch 4 | iter 285 step 17 | loss train: 1.300, val: n/a | iter time: 395.95 ms\n",
      "Epoch 4 | iter 286 step 17 | loss train: 1.299, val: n/a | iter time: 338.38 ms\n",
      "Epoch 4 | iter 287 step 17 | loss train: 1.315, val: n/a | iter time: 387.37 ms\n",
      "Epoch 4 | iter 288 step 18 | loss train: 1.338, val: n/a | iter time: 388.56 ms (step)\n",
      "Epoch 4 | iter 289 step 18 | loss train: 1.357, val: n/a | iter time: 391.43 ms\n",
      "Epoch 4 | iter 290 step 18 | loss train: 1.365, val: n/a | iter time: 347.62 ms\n",
      "Epoch 4 | iter 291 step 18 | loss train: 1.339, val: n/a | iter time: 331.35 ms\n",
      "Epoch 4 | iter 292 step 18 | loss train: 1.307, val: n/a | iter time: 538.63 ms\n",
      "Epoch 4 | iter 293 step 18 | loss train: 1.292, val: n/a | iter time: 659.64 ms\n",
      "Epoch 4 | iter 294 step 18 | loss train: 1.297, val: n/a | iter time: 393.22 ms\n",
      "Epoch 4 | iter 295 step 18 | loss train: 1.290, val: n/a | iter time: 383.54 ms\n",
      "Epoch 4 | iter 296 step 18 | loss train: 1.291, val: n/a | iter time: 499.71 ms\n",
      "Epoch 4 | iter 297 step 18 | loss train: 1.305, val: n/a | iter time: 441.15 ms\n",
      "Epoch 4 | iter 298 step 18 | loss train: 1.299, val: n/a | iter time: 337.50 ms\n",
      "Epoch 4 | iter 299 step 18 | loss train: 1.281, val: n/a | iter time: 436.06 ms\n",
      "Epoch 4 | iter 300 step 18 | loss train: 1.267, val: n/a | iter time: 407.19 ms\n",
      "Epoch 4 | iter 301 step 18 | loss train: 1.289, val: n/a | iter time: 487.97 ms\n",
      "Epoch 4 | iter 302 step 18 | loss train: 1.325, val: n/a | iter time: 308.25 ms\n",
      "Epoch 4 | iter 303 step 18 | loss train: 1.325, val: n/a | iter time: 379.18 ms\n",
      "Epoch 4 | iter 304 step 19 | loss train: 1.317, val: n/a | iter time: 474.22 ms (step)\n",
      "Epoch 4 | iter 305 step 19 | loss train: 1.302, val: n/a | iter time: 429.39 ms\n",
      "Epoch 4 | iter 306 step 19 | loss train: 1.286, val: n/a | iter time: 444.55 ms\n",
      "Epoch 4 | iter 307 step 19 | loss train: 1.285, val: n/a | iter time: 444.42 ms\n",
      "Epoch 4 | iter 308 step 19 | loss train: 1.289, val: n/a | iter time: 499.25 ms\n",
      "Epoch 4 | iter 309 step 19 | loss train: 1.308, val: n/a | iter time: 492.06 ms\n",
      "Epoch 4 | iter 310 step 19 | loss train: 1.308, val: n/a | iter time: 451.89 ms\n",
      "Epoch 4 | iter 311 step 19 | loss train: 1.305, val: n/a | iter time: 354.11 ms\n",
      "Epoch 4 | iter 312 step 19 | loss train: 1.304, val: n/a | iter time: 504.72 ms\n",
      "Epoch 4 | iter 313 step 19 | loss train: 1.304, val: n/a | iter time: 352.61 ms\n",
      "Epoch 4 | iter 314 step 19 | loss train: 1.310, val: n/a | iter time: 412.89 ms\n",
      "Epoch 4 | iter 315 step 19 | loss train: 1.283, val: n/a | iter time: 579.13 ms\n",
      "Epoch 4 | iter 316 step 19 | loss train: 1.286, val: n/a | iter time: 306.83 ms\n",
      "Epoch 4 | iter 317 step 19 | loss train: 1.253, val: n/a | iter time: 581.13 ms\n",
      "Epoch 4 | iter 318 step 19 | loss train: 1.233, val: n/a | iter time: 412.20 ms\n",
      "Epoch 4 | iter 319 step 19 | loss train: 1.242, val: n/a | iter time: 434.13 ms\n",
      "Epoch 4 | iter 320 step 20 | loss train: 1.234, val: n/a | iter time: 520.97 ms (step)\n",
      "Epoch 4 | iter 321 step 20 | loss train: 1.236, val: n/a | iter time: 363.39 ms\n",
      "Epoch 4 | iter 322 step 20 | loss train: 1.231, val: n/a | iter time: 398.73 ms\n",
      "Epoch 4 | iter 323 step 20 | loss train: 1.229, val: n/a | iter time: 481.64 ms\n",
      "Epoch 4 | iter 324 step 20 | loss train: 1.265, val: n/a | iter time: 545.03 ms\n",
      "Epoch 4 | iter 325 step 20 | loss train: 1.245, val: n/a | iter time: 677.98 ms\n",
      "Epoch 4 | iter 326 step 20 | loss train: 1.239, val: n/a | iter time: 643.10 ms\n",
      "Epoch 4 | iter 327 step 20 | loss train: 1.228, val: n/a | iter time: 368.35 ms\n",
      "Epoch 4 | iter 328 step 20 | loss train: 1.240, val: n/a | iter time: 451.49 ms\n",
      "Epoch 4 | iter 329 step 20 | loss train: 1.220, val: n/a | iter time: 538.13 ms\n",
      "Epoch 4 | iter 330 step 20 | loss train: 1.209, val: n/a | iter time: 469.88 ms\n",
      "Epoch 4 | iter 331 step 20 | loss train: 1.208, val: n/a | iter time: 465.17 ms\n",
      "Epoch 4 | iter 332 step 20 | loss train: 1.214, val: n/a | iter time: 483.08 ms\n",
      "Epoch 4 | iter 333 step 20 | loss train: 1.216, val: n/a | iter time: 534.79 ms\n",
      "Epoch 4 | iter 334 step 20 | loss train: 1.210, val: n/a | iter time: 406.50 ms\n",
      "Epoch 4 | iter 335 step 20 | loss train: 1.203, val: n/a | iter time: 357.65 ms\n",
      "Epoch 4 | iter 336 step 21 | loss train: 1.206, val: n/a | iter time: 417.75 ms (step)\n",
      "Epoch 4 | iter 337 step 21 | loss train: 1.213, val: n/a | iter time: 523.29 ms\n",
      "Epoch 4 | iter 338 step 21 | loss train: 1.232, val: n/a | iter time: 409.40 ms\n",
      "Epoch 4 | iter 339 step 21 | loss train: 1.261, val: n/a | iter time: 435.20 ms\n",
      "Epoch 4 | iter 340 step 21 | loss train: 1.246, val: n/a | iter time: 479.31 ms\n",
      "Epoch 4 | iter 341 step 21 | loss train: 1.255, val: n/a | iter time: 636.76 ms\n",
      "Epoch 4 | iter 342 step 21 | loss train: 1.250, val: n/a | iter time: 558.86 ms\n",
      "Epoch 4 | iter 343 step 21 | loss train: 1.245, val: n/a | iter time: 521.71 ms\n",
      "Epoch 4 | iter 344 step 21 | loss train: 1.221, val: n/a | iter time: 606.11 ms\n",
      "Epoch 4 | iter 345 step 21 | loss train: 1.250, val: n/a | iter time: 498.85 ms\n",
      "Epoch 4 | iter 346 step 21 | loss train: 1.254, val: n/a | iter time: 434.10 ms\n",
      "Epoch 4 | iter 347 step 21 | loss train: 1.261, val: n/a | iter time: 444.03 ms\n",
      "Epoch 4 | iter 348 step 21 | loss train: 1.272, val: n/a | iter time: 478.48 ms\n",
      "Epoch 4 | iter 349 step 21 | loss train: 1.274, val: n/a | iter time: 361.64 ms\n",
      "Epoch 4 | iter 350 step 21 | loss train: 1.262, val: n/a | iter time: 450.61 ms\n",
      "Epoch 4 | iter 351 step 21 | loss train: 1.249, val: n/a | iter time: 428.93 ms\n",
      "Epoch 4 | iter 352 step 22 | loss train: 1.266, val: n/a | iter time: 449.78 ms (step)\n",
      "Epoch 4 | iter 353 step 22 | loss train: 1.239, val: n/a | iter time: 505.78 ms\n",
      "Epoch 4 | iter 354 step 22 | loss train: 1.235, val: n/a | iter time: 506.75 ms\n",
      "Epoch 4 | iter 355 step 22 | loss train: 1.200, val: n/a | iter time: 517.35 ms\n",
      "Epoch 4 | iter 356 step 22 | loss train: 1.200, val: n/a | iter time: 497.53 ms\n",
      "Epoch 4 | iter 357 step 22 | loss train: 1.217, val: n/a | iter time: 378.02 ms\n",
      "Epoch 4 | iter 358 step 22 | loss train: 1.232, val: n/a | iter time: 490.95 ms\n",
      "Epoch 4 | iter 359 step 22 | loss train: 1.243, val: n/a | iter time: 730.51 ms\n",
      "Epoch 4 | iter 360 step 22 | loss train: 1.238, val: n/a | iter time: 323.68 ms\n",
      "Epoch 5 | iter 361 step 22 | loss train: 1.201, val: n/a | iter time: 29418.52 ms\n",
      "Epoch 5 | iter 362 step 22 | loss train: 1.213, val: n/a | iter time: 444.60 ms\n",
      "Epoch 5 | iter 363 step 22 | loss train: 1.201, val: n/a | iter time: 428.91 ms\n",
      "Epoch 5 | iter 364 step 22 | loss train: 1.191, val: n/a | iter time: 334.93 ms\n",
      "Epoch 5 | iter 365 step 22 | loss train: 1.219, val: n/a | iter time: 355.08 ms\n",
      "Epoch 5 | iter 366 step 22 | loss train: 1.211, val: n/a | iter time: 429.27 ms\n",
      "Epoch 5 | iter 367 step 22 | loss train: 1.219, val: n/a | iter time: 426.33 ms\n",
      "Epoch 5 | iter 368 step 23 | loss train: 1.207, val: n/a | iter time: 449.74 ms (step)\n",
      "Epoch 5 | iter 369 step 23 | loss train: 1.242, val: n/a | iter time: 354.33 ms\n",
      "Epoch 5 | iter 370 step 23 | loss train: 1.252, val: n/a | iter time: 446.49 ms\n",
      "Epoch 5 | iter 371 step 23 | loss train: 1.274, val: n/a | iter time: 390.33 ms\n",
      "Epoch 5 | iter 372 step 23 | loss train: 1.281, val: n/a | iter time: 329.81 ms\n",
      "Epoch 5 | iter 373 step 23 | loss train: 1.265, val: n/a | iter time: 389.44 ms\n",
      "Epoch 5 | iter 374 step 23 | loss train: 1.241, val: n/a | iter time: 490.84 ms\n",
      "Epoch 5 | iter 375 step 23 | loss train: 1.234, val: n/a | iter time: 568.84 ms\n",
      "Epoch 5 | iter 376 step 23 | loss train: 1.227, val: n/a | iter time: 522.88 ms\n",
      "Epoch 5 | iter 377 step 23 | loss train: 1.243, val: n/a | iter time: 290.05 ms\n",
      "Epoch 5 | iter 378 step 23 | loss train: 1.222, val: n/a | iter time: 391.16 ms\n",
      "Epoch 5 | iter 379 step 23 | loss train: 1.254, val: n/a | iter time: 423.95 ms\n",
      "Epoch 5 | iter 380 step 23 | loss train: 1.248, val: n/a | iter time: 458.05 ms\n",
      "Epoch 5 | iter 381 step 23 | loss train: 1.225, val: n/a | iter time: 452.35 ms\n",
      "Epoch 5 | iter 382 step 23 | loss train: 1.253, val: n/a | iter time: 445.86 ms\n",
      "Epoch 5 | iter 383 step 23 | loss train: 1.254, val: n/a | iter time: 382.22 ms\n",
      "Epoch 5 | iter 384 step 24 | loss train: 1.242, val: n/a | iter time: 307.03 ms (step)\n",
      "Epoch 5 | iter 385 step 24 | loss train: 1.211, val: n/a | iter time: 452.57 ms\n",
      "Epoch 5 | iter 386 step 24 | loss train: 1.182, val: n/a | iter time: 394.19 ms\n",
      "Epoch 5 | iter 387 step 24 | loss train: 1.186, val: n/a | iter time: 429.68 ms\n",
      "Epoch 5 | iter 388 step 24 | loss train: 1.172, val: n/a | iter time: 626.52 ms\n",
      "Epoch 5 | iter 389 step 24 | loss train: 1.191, val: n/a | iter time: 392.07 ms\n",
      "Epoch 5 | iter 390 step 24 | loss train: 1.225, val: n/a | iter time: 342.23 ms\n",
      "Epoch 5 | iter 391 step 24 | loss train: 1.225, val: n/a | iter time: 446.52 ms\n",
      "Epoch 5 | iter 392 step 24 | loss train: 1.251, val: n/a | iter time: 511.40 ms\n",
      "Epoch 5 | iter 393 step 24 | loss train: 1.254, val: n/a | iter time: 449.45 ms\n",
      "Epoch 5 | iter 394 step 24 | loss train: 1.258, val: n/a | iter time: 410.26 ms\n",
      "Epoch 5 | iter 395 step 24 | loss train: 1.248, val: n/a | iter time: 402.79 ms\n",
      "Epoch 5 | iter 396 step 24 | loss train: 1.240, val: n/a | iter time: 557.33 ms\n",
      "Epoch 5 | iter 397 step 24 | loss train: 1.256, val: n/a | iter time: 403.77 ms\n",
      "Epoch 5 | iter 398 step 24 | loss train: 1.260, val: n/a | iter time: 422.34 ms\n",
      "Epoch 5 | iter 399 step 24 | loss train: 1.269, val: n/a | iter time: 532.98 ms\n",
      "Epoch 5 | iter 400 step 25 | loss train: 1.278, val: n/a | iter time: 722.51 ms (step)\n",
      "Epoch 5 | iter 401 step 25 | loss train: 1.294, val: n/a | iter time: 406.24 ms\n",
      "Epoch 5 | iter 402 step 25 | loss train: 1.294, val: n/a | iter time: 365.78 ms\n",
      "Epoch 5 | iter 403 step 25 | loss train: 1.279, val: n/a | iter time: 413.42 ms\n",
      "Epoch 5 | iter 404 step 25 | loss train: 1.305, val: n/a | iter time: 484.46 ms\n",
      "Epoch 5 | iter 405 step 25 | loss train: 1.286, val: n/a | iter time: 598.26 ms\n",
      "Epoch 5 | iter 406 step 25 | loss train: 1.258, val: n/a | iter time: 551.85 ms\n",
      "Epoch 5 | iter 407 step 25 | loss train: 1.260, val: n/a | iter time: 389.07 ms\n",
      "Epoch 5 | iter 408 step 25 | loss train: 1.228, val: n/a | iter time: 636.45 ms\n",
      "Epoch 5 | iter 409 step 25 | loss train: 1.222, val: n/a | iter time: 451.16 ms\n",
      "Epoch 5 | iter 410 step 25 | loss train: 1.232, val: n/a | iter time: 285.76 ms\n",
      "Epoch 5 | iter 411 step 25 | loss train: 1.231, val: n/a | iter time: 518.74 ms\n",
      "Epoch 5 | iter 412 step 25 | loss train: 1.239, val: n/a | iter time: 508.83 ms\n",
      "Epoch 5 | iter 413 step 25 | loss train: 1.247, val: n/a | iter time: 439.35 ms\n",
      "Epoch 5 | iter 414 step 25 | loss train: 1.229, val: n/a | iter time: 475.22 ms\n",
      "Epoch 5 | iter 415 step 25 | loss train: 1.228, val: n/a | iter time: 455.03 ms\n",
      "Epoch 5 | iter 416 step 26 | loss train: 1.233, val: n/a | iter time: 405.96 ms (step)\n",
      "Epoch 5 | iter 417 step 26 | loss train: 1.234, val: n/a | iter time: 563.45 ms\n",
      "Epoch 5 | iter 418 step 26 | loss train: 1.245, val: n/a | iter time: 517.24 ms\n",
      "Epoch 5 | iter 419 step 26 | loss train: 1.249, val: n/a | iter time: 447.50 ms\n",
      "Epoch 5 | iter 420 step 26 | loss train: 1.236, val: n/a | iter time: 619.87 ms\n",
      "Epoch 5 | iter 421 step 26 | loss train: 1.234, val: n/a | iter time: 639.09 ms\n",
      "Epoch 5 | iter 422 step 26 | loss train: 1.252, val: n/a | iter time: 397.12 ms\n",
      "Epoch 5 | iter 423 step 26 | loss train: 1.261, val: n/a | iter time: 500.84 ms\n",
      "Epoch 5 | iter 424 step 26 | loss train: 1.280, val: n/a | iter time: 380.80 ms\n",
      "Epoch 5 | iter 425 step 26 | loss train: 1.276, val: n/a | iter time: 501.24 ms\n",
      "Epoch 5 | iter 426 step 26 | loss train: 1.285, val: n/a | iter time: 589.60 ms\n",
      "Epoch 5 | iter 427 step 26 | loss train: 1.265, val: n/a | iter time: 365.66 ms\n",
      "Epoch 5 | iter 428 step 26 | loss train: 1.247, val: n/a | iter time: 765.65 ms\n",
      "Epoch 5 | iter 429 step 26 | loss train: 1.235, val: n/a | iter time: 387.16 ms\n",
      "Epoch 5 | iter 430 step 26 | loss train: 1.232, val: n/a | iter time: 631.20 ms\n",
      "Epoch 5 | iter 431 step 26 | loss train: 1.224, val: n/a | iter time: 572.35 ms\n",
      "Epoch 5 | iter 432 step 27 | loss train: 1.235, val: n/a | iter time: 575.78 ms (step)\n",
      "Epoch 5 | iter 433 step 27 | loss train: 1.252, val: n/a | iter time: 390.07 ms\n",
      "Epoch 5 | iter 434 step 27 | loss train: 1.285, val: n/a | iter time: 361.18 ms\n",
      "Epoch 5 | iter 435 step 27 | loss train: 1.275, val: n/a | iter time: 498.18 ms\n",
      "Epoch 5 | iter 436 step 27 | loss train: 1.265, val: n/a | iter time: 454.68 ms\n",
      "Epoch 5 | iter 437 step 27 | loss train: 1.277, val: n/a | iter time: 429.16 ms\n",
      "Epoch 5 | iter 438 step 27 | loss train: 1.273, val: n/a | iter time: 448.48 ms\n",
      "Epoch 5 | iter 439 step 27 | loss train: 1.300, val: n/a | iter time: 368.06 ms\n",
      "Epoch 5 | iter 440 step 27 | loss train: 1.309, val: n/a | iter time: 449.15 ms\n",
      "Epoch 5 | iter 441 step 27 | loss train: 1.329, val: n/a | iter time: 441.14 ms\n",
      "Epoch 5 | iter 442 step 27 | loss train: 1.296, val: n/a | iter time: 743.98 ms\n",
      "Epoch 5 | iter 443 step 27 | loss train: 1.298, val: n/a | iter time: 460.16 ms\n",
      "Epoch 5 | iter 444 step 27 | loss train: 1.315, val: n/a | iter time: 511.77 ms\n",
      "Epoch 5 | iter 445 step 27 | loss train: 1.299, val: n/a | iter time: 594.71 ms\n",
      "Epoch 5 | iter 446 step 27 | loss train: 1.314, val: n/a | iter time: 447.29 ms\n",
      "Epoch 5 | iter 447 step 27 | loss train: 1.309, val: n/a | iter time: 511.56 ms\n",
      "Epoch 5 | iter 448 step 28 | loss train: 1.294, val: n/a | iter time: 547.21 ms (step)\n",
      "Epoch 5 | iter 449 step 28 | loss train: 1.288, val: n/a | iter time: 517.24 ms\n",
      "Epoch 5 | iter 450 step 28 | loss train: 1.272, val: n/a | iter time: 480.07 ms\n",
      "\n",
      "| ------------------------------------------------------\n",
      "| Token Counts\n",
      "| - Input Tokens              :  225275\n",
      "| - Tokens w/ Prompt          :  236075\n",
      "| - Total Tokens (w/ Padding) :  236075\n",
      "| -----------------------------------------------------\n",
      "| Performance\n",
      "| - Training Time             :  401.98 s\n",
      "| - Tok/sec                   :  587.28 tok/s\n",
      "| -----------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "\n",
      "Validating ...\n",
      "Final evaluation | val loss: 1.260 | val ppl: 3.524\n",
      "Saving LoRA weights to 'out/llama-custom-model/final/lit_model.pth.lora'\n",
      "{'checkpoint_dir': PosixPath('out/llama-custom-model/final'),\n",
      " 'precision': None,\n",
      " 'pretrained_checkpoint_dir': None}\n",
      "/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/litgpt/scripts/merge_lora.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_checkpoint = torch.load(str(pretrained_checkpoint_dir / \"lit_model.pth\"), mmap=True)\n",
      "/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/litgpt/scripts/merge_lora.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lora_checkpoint = torch.load(str(lora_path), mmap=True)\n",
      "Saved merged weights to 'out/llama-custom-model/final/lit_model.pth'\n"
     ]
    }
   ],
   "source": [
    "!litgpt finetune_lora meta-llama/Llama-3.2-1B-Instruct \\\n",
    "  --data JSON \\\n",
    "  --data.json_path my_custom_dataset_small.json  \\\n",
    "  --data.val_split_fraction 0.1 \\\n",
    "  --out_dir out/llama-custom-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evalute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'access_token': None,\n",
      " 'batch_size': 4,\n",
      " 'checkpoint_dir': PosixPath('out/llama-custom-model/final'),\n",
      " 'device': None,\n",
      " 'dtype': None,\n",
      " 'force_conversion': False,\n",
      " 'limit': None,\n",
      " 'num_fewshot': None,\n",
      " 'out_dir': PosixPath('llama_custom_model_eval'),\n",
      " 'save_filepath': None,\n",
      " 'seed': 1234,\n",
      " 'tasks': 'truthfulqa_mc2,mmlu'}\n",
      "{'checkpoint_dir': PosixPath('out/llama-custom-model/final'),\n",
      " 'output_dir': PosixPath('llama_custom_model_eval')}\n",
      "/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/litgpt/eval/evaluate.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(out_dir / \"model.pth\")\n",
      "2025-01-04:20:15:40,746 INFO     [huggingface.py:132] Using device 'cpu'\n",
      "2025-01-04:20:15:40,973 INFO     [huggingface.py:369] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cpu'}\n",
      "2025-01-04:20:15:41,260 INFO     [evaluator.py:164] Setting random seed to 1234 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-01-04:20:15:41,260 INFO     [evaluator.py:217] Using pre-initialized model\n",
      "2025-01-04:20:17:25,771 INFO     [task.py:415] Building contexts for mmlu_college_chemistry on rank 0...\n",
      "100%|███████████████████████████████████████| 100/100 [00:00<00:00, 1513.54it/s]\n",
      "2025-01-04:20:17:25,840 INFO     [task.py:415] Building contexts for mmlu_high_school_statistics on rank 0...\n",
      "100%|███████████████████████████████████████| 216/216 [00:00<00:00, 1779.52it/s]\n",
      "2025-01-04:20:17:25,966 INFO     [task.py:415] Building contexts for mmlu_machine_learning on rank 0...\n",
      "100%|███████████████████████████████████████| 112/112 [00:00<00:00, 1785.62it/s]\n",
      "2025-01-04:20:17:26,031 INFO     [task.py:415] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
      "100%|███████████████████████████████████████| 203/203 [00:00<00:00, 1758.14it/s]\n",
      "2025-01-04:20:17:26,150 INFO     [task.py:415] Building contexts for mmlu_college_mathematics on rank 0...\n",
      "100%|███████████████████████████████████████| 100/100 [00:00<00:00, 1785.36it/s]\n",
      "2025-01-04:20:17:26,208 INFO     [task.py:415] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
      "100%|███████████████████████████████████████| 270/270 [00:00<00:00, 1789.74it/s]\n",
      "2025-01-04:20:17:26,364 INFO     [task.py:415] Building contexts for mmlu_computer_security on rank 0...\n",
      "100%|███████████████████████████████████████| 100/100 [00:00<00:00, 1763.32it/s]\n",
      "2025-01-04:20:17:26,423 INFO     [task.py:415] Building contexts for mmlu_conceptual_physics on rank 0...\n",
      "100%|███████████████████████████████████████| 235/235 [00:00<00:00, 1766.29it/s]\n",
      "2025-01-04:20:17:26,561 INFO     [task.py:415] Building contexts for mmlu_college_computer_science on rank 0...\n",
      "100%|███████████████████████████████████████| 100/100 [00:00<00:00, 1778.25it/s]\n",
      "2025-01-04:20:17:26,619 INFO     [task.py:415] Building contexts for mmlu_high_school_biology on rank 0...\n",
      "100%|███████████████████████████████████████| 310/310 [00:00<00:00, 1769.42it/s]\n",
      "2025-01-04:20:17:26,801 INFO     [task.py:415] Building contexts for mmlu_high_school_physics on rank 0...\n",
      "100%|███████████████████████████████████████| 151/151 [00:00<00:00, 1791.67it/s]\n",
      "2025-01-04:20:17:26,888 INFO     [task.py:415] Building contexts for mmlu_electrical_engineering on rank 0...\n",
      "100%|███████████████████████████████████████| 145/145 [00:00<00:00, 1765.94it/s]\n",
      "2025-01-04:20:17:26,973 INFO     [task.py:415] Building contexts for mmlu_anatomy on rank 0...\n",
      "100%|███████████████████████████████████████| 135/135 [00:00<00:00, 1761.64it/s]\n",
      "2025-01-04:20:17:27,052 INFO     [task.py:415] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
      "100%|███████████████████████████████████████| 100/100 [00:00<00:00, 1783.58it/s]\n",
      "2025-01-04:20:17:27,110 INFO     [task.py:415] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
      "100%|███████████████████████████████████████| 378/378 [00:00<00:00, 1784.68it/s]\n",
      "2025-01-04:20:17:27,329 INFO     [task.py:415] Building contexts for mmlu_college_physics on rank 0...\n",
      "100%|███████████████████████████████████████| 102/102 [00:00<00:00, 1773.55it/s]\n",
      "2025-01-04:20:17:27,388 INFO     [task.py:415] Building contexts for mmlu_college_biology on rank 0...\n",
      "100%|███████████████████████████████████████| 144/144 [00:00<00:00, 1770.22it/s]\n",
      "2025-01-04:20:17:27,473 INFO     [task.py:415] Building contexts for mmlu_astronomy on rank 0...\n",
      "100%|███████████████████████████████████████| 152/152 [00:00<00:00, 1775.08it/s]\n",
      "2025-01-04:20:17:27,561 INFO     [task.py:415] Building contexts for mmlu_abstract_algebra on rank 0...\n",
      "100%|███████████████████████████████████████| 100/100 [00:00<00:00, 1783.68it/s]\n",
      "2025-01-04:20:17:27,620 INFO     [task.py:415] Building contexts for mmlu_management on rank 0...\n",
      "100%|███████████████████████████████████████| 103/103 [00:00<00:00, 1674.42it/s]\n",
      "2025-01-04:20:17:27,683 INFO     [task.py:415] Building contexts for mmlu_miscellaneous on rank 0...\n",
      "100%|███████████████████████████████████████| 783/783 [00:00<00:00, 1800.00it/s]\n",
      "2025-01-04:20:17:28,133 INFO     [task.py:415] Building contexts for mmlu_human_aging on rank 0...\n",
      "100%|███████████████████████████████████████| 223/223 [00:00<00:00, 1765.08it/s]\n",
      "2025-01-04:20:17:28,264 INFO     [task.py:415] Building contexts for mmlu_virology on rank 0...\n",
      "100%|███████████████████████████████████████| 166/166 [00:00<00:00, 1788.29it/s]\n",
      "2025-01-04:20:17:28,360 INFO     [task.py:415] Building contexts for mmlu_nutrition on rank 0...\n",
      "100%|███████████████████████████████████████| 306/306 [00:00<00:00, 1778.78it/s]\n",
      "2025-01-04:20:17:28,538 INFO     [task.py:415] Building contexts for mmlu_marketing on rank 0...\n",
      "100%|███████████████████████████████████████| 234/234 [00:00<00:00, 1752.32it/s]\n",
      "2025-01-04:20:17:28,676 INFO     [task.py:415] Building contexts for mmlu_professional_medicine on rank 0...\n",
      "100%|███████████████████████████████████████| 272/272 [00:00<00:00, 1773.07it/s]\n",
      "2025-01-04:20:17:28,835 INFO     [task.py:415] Building contexts for mmlu_business_ethics on rank 0...\n",
      "100%|███████████████████████████████████████| 100/100 [00:00<00:00, 1750.46it/s]\n",
      "2025-01-04:20:17:28,894 INFO     [task.py:415] Building contexts for mmlu_professional_accounting on rank 0...\n",
      "100%|███████████████████████████████████████| 282/282 [00:00<00:00, 1779.83it/s]\n",
      "2025-01-04:20:17:29,058 INFO     [task.py:415] Building contexts for mmlu_global_facts on rank 0...\n",
      "100%|███████████████████████████████████████| 100/100 [00:00<00:00, 1783.26it/s]\n",
      "2025-01-04:20:17:29,116 INFO     [task.py:415] Building contexts for mmlu_medical_genetics on rank 0...\n",
      "100%|███████████████████████████████████████| 100/100 [00:00<00:00, 1776.89it/s]\n",
      "2025-01-04:20:17:29,174 INFO     [task.py:415] Building contexts for mmlu_clinical_knowledge on rank 0...\n",
      "100%|███████████████████████████████████████| 265/265 [00:00<00:00, 1802.34it/s]\n",
      "2025-01-04:20:17:29,326 INFO     [task.py:415] Building contexts for mmlu_college_medicine on rank 0...\n",
      "100%|████████████████████████████████████████| 173/173 [00:00<00:00, 981.29it/s]\n",
      "2025-01-04:20:17:29,506 INFO     [task.py:415] Building contexts for mmlu_econometrics on rank 0...\n",
      "100%|███████████████████████████████████████| 114/114 [00:00<00:00, 1784.74it/s]\n",
      "2025-01-04:20:17:29,572 INFO     [task.py:415] Building contexts for mmlu_sociology on rank 0...\n",
      "100%|███████████████████████████████████████| 201/201 [00:00<00:00, 1771.69it/s]\n",
      "2025-01-04:20:17:29,690 INFO     [task.py:415] Building contexts for mmlu_professional_psychology on rank 0...\n",
      "100%|███████████████████████████████████████| 612/612 [00:00<00:00, 1790.44it/s]\n",
      "2025-01-04:20:17:30,043 INFO     [task.py:415] Building contexts for mmlu_high_school_geography on rank 0...\n",
      "100%|███████████████████████████████████████| 198/198 [00:00<00:00, 1794.49it/s]\n",
      "2025-01-04:20:17:30,157 INFO     [task.py:415] Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
      "100%|███████████████████████████████████████| 238/238 [00:00<00:00, 1785.51it/s]\n",
      "2025-01-04:20:17:30,294 INFO     [task.py:415] Building contexts for mmlu_high_school_psychology on rank 0...\n",
      "100%|███████████████████████████████████████| 545/545 [00:00<00:00, 1794.43it/s]\n",
      "2025-01-04:20:17:30,608 INFO     [task.py:415] Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
      "100%|███████████████████████████████████████| 193/193 [00:00<00:00, 1776.83it/s]\n",
      "2025-01-04:20:17:30,720 INFO     [task.py:415] Building contexts for mmlu_us_foreign_policy on rank 0...\n",
      "100%|███████████████████████████████████████| 100/100 [00:00<00:00, 1786.26it/s]\n",
      "2025-01-04:20:17:30,778 INFO     [task.py:415] Building contexts for mmlu_human_sexuality on rank 0...\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 1799.06it/s]\n",
      "2025-01-04:20:17:30,854 INFO     [task.py:415] Building contexts for mmlu_security_studies on rank 0...\n",
      "100%|███████████████████████████████████████| 245/245 [00:00<00:00, 1802.66it/s]\n",
      "2025-01-04:20:17:30,994 INFO     [task.py:415] Building contexts for mmlu_public_relations on rank 0...\n",
      "100%|███████████████████████████████████████| 110/110 [00:00<00:00, 1794.16it/s]\n",
      "2025-01-04:20:17:31,058 INFO     [task.py:415] Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
      "100%|███████████████████████████████████████| 390/390 [00:00<00:00, 1794.77it/s]\n",
      "2025-01-04:20:17:31,282 INFO     [task.py:415] Building contexts for mmlu_prehistory on rank 0...\n",
      "100%|███████████████████████████████████████| 324/324 [00:00<00:00, 1803.67it/s]\n",
      "2025-01-04:20:17:31,468 INFO     [task.py:415] Building contexts for mmlu_moral_disputes on rank 0...\n",
      "100%|███████████████████████████████████████| 346/346 [00:00<00:00, 1782.07it/s]\n",
      "2025-01-04:20:17:31,668 INFO     [task.py:415] Building contexts for mmlu_international_law on rank 0...\n",
      "100%|███████████████████████████████████████| 121/121 [00:00<00:00, 1793.20it/s]\n",
      "2025-01-04:20:17:31,738 INFO     [task.py:415] Building contexts for mmlu_high_school_us_history on rank 0...\n",
      "100%|███████████████████████████████████████| 204/204 [00:00<00:00, 1784.84it/s]\n",
      "2025-01-04:20:17:31,856 INFO     [task.py:415] Building contexts for mmlu_high_school_world_history on rank 0...\n",
      "100%|███████████████████████████████████████| 237/237 [00:00<00:00, 1806.67it/s]\n",
      "2025-01-04:20:17:31,992 INFO     [task.py:415] Building contexts for mmlu_world_religions on rank 0...\n",
      "100%|███████████████████████████████████████| 171/171 [00:00<00:00, 1811.74it/s]\n",
      "2025-01-04:20:17:32,090 INFO     [task.py:415] Building contexts for mmlu_professional_law on rank 0...\n",
      "100%|█████████████████████████████████████| 1534/1534 [00:00<00:00, 1784.23it/s]\n",
      "2025-01-04:20:17:32,977 INFO     [task.py:415] Building contexts for mmlu_philosophy on rank 0...\n",
      "100%|███████████████████████████████████████| 311/311 [00:00<00:00, 1765.48it/s]\n",
      "2025-01-04:20:17:33,159 INFO     [task.py:415] Building contexts for mmlu_logical_fallacies on rank 0...\n",
      "100%|███████████████████████████████████████| 163/163 [00:00<00:00, 1787.65it/s]\n",
      "2025-01-04:20:17:33,254 INFO     [task.py:415] Building contexts for mmlu_high_school_european_history on rank 0...\n",
      "100%|███████████████████████████████████████| 165/165 [00:00<00:00, 1754.46it/s]\n",
      "2025-01-04:20:17:33,351 INFO     [task.py:415] Building contexts for mmlu_formal_logic on rank 0...\n",
      "100%|███████████████████████████████████████| 126/126 [00:00<00:00, 1795.99it/s]\n",
      "2025-01-04:20:17:33,424 INFO     [task.py:415] Building contexts for mmlu_moral_scenarios on rank 0...\n",
      "100%|███████████████████████████████████████| 895/895 [00:00<00:00, 1783.07it/s]\n",
      "2025-01-04:20:17:33,941 INFO     [task.py:415] Building contexts for mmlu_jurisprudence on rank 0...\n",
      "100%|███████████████████████████████████████| 108/108 [00:00<00:00, 1769.31it/s]\n",
      "2025-01-04:20:17:34,005 INFO     [task.py:415] Building contexts for truthfulqa_mc2 on rank 0...\n",
      "100%|███████████████████████████████████████| 817/817 [00:00<00:00, 2054.70it/s]\n",
      "2025-01-04:20:17:34,427 INFO     [evaluator.py:496] Running loglikelihood requests\n",
      "Running loglikelihood requests:   8%|▎   | 4961/62050 [10:21<1:41:18,  9.39it/s]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/fine-tune/bin/litgpt\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/litgpt/__main__.py\", line 71, in main\n",
      "    CLI(parser_data)\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/jsonargparse/_cli.py\", line 119, in CLI\n",
      "    return _run_component(component, init.get(subcommand))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/jsonargparse/_cli.py\", line 204, in _run_component\n",
      "    return component(**cfg)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/litgpt/eval/evaluate.py\", line 110, in convert_and_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/lm_eval/utils.py\", line 401, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/lm_eval/evaluator.py\", line 303, in simple_evaluate\n",
      "    results = evaluate(\n",
      "              ^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/lm_eval/utils.py\", line 401, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/lm_eval/evaluator.py\", line 507, in evaluate\n",
      "    resps = getattr(lm, reqtype)(cloned_reqs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/lm_eval/api/model.py\", line 378, in loglikelihood\n",
      "    return self._loglikelihood_tokens(new_reqs, disable_tqdm=disable_tqdm)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/lm_eval/models/huggingface.py\", line 1166, in _loglikelihood_tokens\n",
      "    self._model_call(batched_inps, **call_kwargs), dim=-1\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/lm_eval/models/huggingface.py\", line 856, in _model_call\n",
      "    return self.model(inps).logits\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py\", line 1163, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py\", line 913, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py\", line 655, in forward\n",
      "    hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aakinlalu/.pyenv/versions/3.12.0/envs/fine-tune/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py\", line 73, in forward\n",
      "    variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "               ^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!litgpt evaluate out/llama-custom-model/final \\\n",
    "   --batch_size 4 \\\n",
    "   --tasks 'truthfulqa_mc2,mmlu' \\\n",
    "    --out_dir llama_custom_model_eval/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use the model to answer prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'checkpoint_dir': PosixPath('out/llama-custom-model/final'),\n",
      " 'compile': False,\n",
      " 'max_new_tokens': 50,\n",
      " 'num_samples': 1,\n",
      " 'precision': None,\n",
      " 'prompt': 'What is the best way to invest in the stock market?',\n",
      " 'quantize': None,\n",
      " 'temperature': 0.8,\n",
      " 'top_k': 50,\n",
      " 'top_p': 1.0}\n",
      "Loading model 'out/llama-custom-model/final/lit_model.pth' with {'name': 'Llama-3.2-1B-Instruct', 'hf_config': {'name': 'Llama-3.2-1B-Instruct', 'org': 'meta-llama'}, 'scale_embeddings': False, 'attention_scores_scalar': None, 'block_size': 131072, 'sliding_window_size': None, 'sliding_window_layer_placing': None, 'vocab_size': 128000, 'padding_multiple': 512, 'padded_vocab_size': 128256, 'n_layer': 16, 'n_head': 32, 'head_size': 64, 'n_embd': 2048, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'attn_bias': False, 'n_query_groups': 8, 'shared_attention_norm': False, 'norm_class_name': 'RMSNorm', 'post_attention_norm': False, 'post_mlp_norm': False, 'norm_eps': 1e-05, 'mlp_class_name': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 8192, 'rope_condense_ratio': 1, 'rope_base': 500000, 'rope_adjustments': {'factor': 32.0, 'high_freq_factor': 4.0, 'low_freq_factor': 1.0, 'original_max_seq_len': 8192}, 'n_expert': 0, 'n_expert_per_token': 0, 'attention_logit_softcapping': None, 'final_logit_softcapping': None, 'rope_n_elem': 64}\n",
      "Time to instantiate model: 0.07 seconds.\n",
      "Time to load the model weights: 1.13 seconds.\n",
      "Seed set to 1234\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the best way to invest in the stock market?\n",
      "\n",
      "### Response:\n",
      "While there is no one-size-fits-all answer to this question, investing in the stock market can be a viable option for many individuals. The best way to get started depends on several factors, including your financial goals, risk tolerance, time horizon,\n",
      "Time for inference 1: 1.54 sec total, 32.38 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "!litgpt generate out/llama-custom-model/final --prompt \"What is the best way to invest in the stock market?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Chat with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!litgpt chat out/llama-custom-model/final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!litgpt serve out/llama-custom-model/final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
