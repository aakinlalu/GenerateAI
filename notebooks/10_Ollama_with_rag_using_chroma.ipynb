{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG using chroma and Llama 2 with Ollama API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "path = Path.cwd().parent\n",
    "\n",
    "sys.path.append(str(path))\n",
    "\n",
    "from chromadb_cli.chromadb_cli import load_n_split_doc, load_doc_to_db, get_db, perform_search\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Web Page and split into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "http_path = 'https://lilianweng.github.io/posts/2023-06-23-agent/'\n",
    "docs = load_n_split_doc(http_path, 1500, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Embed and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x286a0ea70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_doc_to_db(docs, collection_name='ollama_exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_search(query='\"How can Task Decomposition be done?', collection_name='ollama_exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "QA_CHAIN_PROMPT  = hub.pull(\"rlm/rag-prompt-llama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM\n",
    "Ensure use ollama to download llama2 \n",
    "```ollama pull llama2```\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager \n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama2\",\n",
    "    verbose=True,\n",
    "    callbacks=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA \n",
    "\n",
    "vectorstore = get_db(collection_name='ollama_exp')\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are several approaches to task decomposition for AI agents, including:\n",
      "\n",
      "1. Chain of thought (CoT): This involves instructing the model to \"think step by step\" to decompose a complex task into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and provides insights into the model's thinking process.\n",
      "2. Tree of thoughts (ToT): This extends CoT by exploring multiple reasoning possibilities at each step, generating multiple thoughts per step, and creating a tree structure. ToT can be used to solve complex problems by searching through the tree structure.\n",
      "3. Using task-specific instructions: This involves providing specific instructions for completing a task, such as \"Write a story outline.\" for writing a novel.\n",
      "4. Human inputs: This involves using human inputs to guide the decomposition of tasks, such as providing detailed instructions or API call context.\n",
      "\n",
      "The limited context length can make it challenging for LLMs to adjust plans when faced with unexpected errors, and the reliability of natural language interface can be questionable due to formatting errors and rebellious behavior. However, vector stores and retrieval can provide access to a larger knowledge pool, but their representation power is not as powerful as full attention."
     ]
    }
   ],
   "source": [
    "question = \"What are the various approaches to Task Decomposition for AI Agents?\"\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can also get logging for tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import LLMResult \n",
    "from langchain.callbacks.base import BaseCallbackHandler \n",
    "\n",
    "\n",
    "class GenerationStaticsCalleback(BaseCallbackHandler):\n",
    "    def on_llm_end(self, result: LLMResult, **kwargs)-> None: \n",
    "        print(result.generations[0][0].generation_info)\n",
    "\n",
    "\n",
    "callback_manager = CallbackManager(\n",
    "        [StreamingStdOutCallbackHandler(), GenerationStaticsCalleback()]\n",
    "    )\n",
    "\n",
    "llm = Ollama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"llama2\",\n",
    "    verbose=True,\n",
    "    callbacks=callback_manager\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")\n",
    "\n",
    "# question = \"What are the various approaches to Task Decomposition for AI Agents?\"\n",
    "# result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Mistral \n",
    "Need to download mistral\n",
    "``\n",
    "ollama pull mistral:7b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_CHAIN_PROMPT= hub.pull(\"rlm/rag-prompt-mistral\")\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"mistral:7b-instruct\",\n",
    "    verbose=True,\n",
    "    callbacks=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The various approaches to Task Decomposition for AI Agents are Chain of Thought (CoT), Tree of Thoughts, and simple prompting or human inputs. CoT involves breaking down a big task into smaller and simpler steps and exploring multiple reasoning possibilities at each step with BFS or DFS search process and evaluation by a classifier or majority vote. Tree of Thoughts extends CoT by generating multiple thoughts per step to create a tree structure, and the search process can also be BFS or DFS. Simple prompting involves using LLM with simple prompts like \"Steps for XYZ\" or human inputs for task-specific instructions like \"Write a story outline.\"\n",
      "\n",
      "Task Decomposition can be done (1) by LLM with simple prompting, (2) by using task-specific instructions, and (3) with human inputs."
     ]
    }
   ],
   "source": [
    "question = \"What are the various approaches to Task Decomposition for AI Agents?\"\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
