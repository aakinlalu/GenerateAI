{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Optimization with DSPy\n",
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "lm = dspy.LM('ollama_chat/llama3.2', api_base='http://localhost:11434')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the LM directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It looks like we're starting fresh. How can I assist you today?\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(\"Say this is a test\", temperature=0.7)\n",
    "# lm(mesaages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the LM with DSPY module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldn't find any information on the number of floors in a castle inherited by David Gregory.\n"
     ]
    }
   ],
   "source": [
    "# Define a module (ChainOfThought)  and assign it a signature ( return an answer, given  a question)\n",
    "qa = dspy.ChainOfThought('question -> answer')\n",
    "\n",
    "# Run with the default LM configured with dspy.configure above\n",
    "response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using multiple LMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is not enough information provided to give a specific number of floors in the castle.\n"
     ]
    }
   ],
   "source": [
    "with dspy.context(lm=dspy.LM('ollama_chat/llama3.3', api_base='http://localhost:11434')):\n",
    "    response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "    print(response.answer)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting output and usage metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prompt', 'messages', 'kwargs', 'response', 'outputs', 'usage', 'cost', 'timestamp', 'uuid', 'model', 'model_type'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm.history)\n",
    "\n",
    "lm.history[-1].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a class-based signature becuase it lets us explicitly specify the categories we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signature\n",
    "A signature is a declarative specification of input/output behavior of a DSPy module. Signatures allow you to tell the LM what it needs to do, rather than specify how we should ask the LM to do it.\n",
    "\n",
    "### Inline DSPy Signatures\n",
    "\n",
    "Signatures can be defined as a short string, with argument names and optional types that define semantic roles for inputs/outputs.\n",
    "\n",
    "1. Question Answering: `question -> answer`, which is equivalent to `question: str -> answer: str`2.  as the default type is always str\n",
    "\n",
    "2. Sentiment Classification: `sentence -> sentiment: bool`, e.g. True if positive\n",
    "\n",
    "3. Summarization: `document -> summary`\n",
    "\n",
    "Your signatures can also have multiple input/output fields with types:\n",
    "\n",
    "4. Retrieval-Augmented Question Answering: `context: list[str], question: str -> answer: str`\n",
    "\n",
    "5. Multiple-Choice Question Answering with Reasoning: `question, choices: list[str] -> reasoning: str, selection: int`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    sentiment=True\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"It's a charming and often affecting journey\"\n",
    "\n",
    "classify = dspy.Predict('sentence -> sentiment: bool')\n",
    "classify(sentence=sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lee is a 21-year-old football player who has played for several teams, including the Hammers, Blackpool, and Colchester United. He currently plays for the promoted Tykes.\n"
     ]
    }
   ],
   "source": [
    "# Example from the XSum dataset.\n",
    "document = \"\"\"The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.\"\"\"\n",
    "\n",
    "summarize = dspy.ChainOfThought('document -> summary')\n",
    "response = summarize(document=document)\n",
    "\n",
    "print(response.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many DSPy modules (except `dspy.Predict`) return auxiliary information by expanding your signature under the hood.\n",
    "\n",
    "For example, `dspy.ChainOfThought` also adds a reasoning field that includes the LM's reasoning before it generates \n",
    "the output summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: The document provides information about a 21-year-old football player named Lee, who has made appearances for several teams including the Hammers, Blackpool, and Colchester United. The document also mentions his loan spells and his current contract with the promoted Tykes.\n"
     ]
    }
   ],
   "source": [
    "print(\"Reasoning:\", response.reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class-based DSPY Signatures\n",
    "\n",
    "For some advanced tasks, you need more verbose signatures. This is typically to:\n",
    "\n",
    "Clarify something about the nature of the task (expressed below as a `docstring`).\n",
    "\n",
    "Supply hints on the nature of an input field, expressed as a `desc` keyword argument for `dspy.InputField`.\n",
    "\n",
    "Supply constraints on an output field, expressed as a `desc` keyword argument for `dspy.OutputField`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    sentiment='fear'\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Literal, List, Dict\n",
    "\n",
    "class Emotion(dspy.Signature):\n",
    "    \"\"\"Classify emotion.\"\"\"\n",
    "\n",
    "    sentence: str = dspy.InputField()\n",
    "    sentiment: Literal['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'] = dspy.OutputField()\n",
    "\n",
    "sentence = \"i started feeling a little vulnerable when the giant spotlight started blinding me\"  # from dair-ai/emotion\n",
    "classify = dspy.Predict(Emotion)\n",
    "classify(sentence=sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:** There's nothing wrong with specifying your requests to the LM more clearly. Class-based Signatures help you with that. However, don't prematurely tune the keywords of your signature by hand. The DSPy optimizers will likely do a better job (and will transfer better across LMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Metric that evaluates faithfulness to citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    faithfulness=False,\n",
       "    evidence={'Claim': ['The text claims that Lee scored 3 goals for Colchester United.'], 'Context': [\"The context states that Lee scored twice for the U's but was unable to save them from relegation.\"]}\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CheckCitationFaithfulness(dspy.Signature):\n",
    "    \"\"\"Verify that the text is based on the provided context.\"\"\"\n",
    "\n",
    "    context: str = dspy.InputField(desc=\"facts here are assumed to be true\")\n",
    "    text: str = dspy.InputField()\n",
    "    faithfulness: bool = dspy.OutputField()\n",
    "    evidence: dict[str, list[str]] = dspy.OutputField(desc=\"Supporting evidence for claims\")\n",
    "\n",
    "context = \"The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.\"\n",
    "\n",
    "text = \"Lee scored 3 goals for Colchester United.\"\n",
    "\n",
    "faithfulness = dspy.Predict(CheckCitationFaithfulness)\n",
    "faithfulness(context=context, text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules\n",
    "\n",
    "A DSPy module is a building block for programs that use LMs.\n",
    "\n",
    "Each built-in module abstracts a prompting technique (like chain of thought or ReAct). Crucially, they are generalized to handle any signature.\n",
    "\n",
    "A DSPy module has learnable parameters (i.e., the little pieces comprising the prompt and the LM weights) and can be invoked (called) to process inputs and return outputs.\n",
    "\n",
    "Multiple modules can be composed into bigger modules (programs). DSPy modules are inspired directly by NN modules in PyTorch, but applied to LM programs.\n",
    "\n",
    "### How do I use a built-in module like `dspy.Predict` or `dspy.ChainOfThought`?\n",
    "\n",
    "**dspy.Predict**\n",
    "Internally, all other DSPy modules are built using dspy.Predict.\n",
    "\n",
    "**dspy.ChainOfThought**\n",
    "When we declare a module, we can pass configuration keys to it.\n",
    "\n",
    "Below, we'll pass `n=5` to request five completions. We can also pass `temperature` or `max_len`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"ColBERT's use of semantic search enables more accurate and informative retrieval results, making it a valuable tool for applications such as information retrieval, question answering, and text classification.\"]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What's something great about the ColBERT retrieval model?\"\n",
    "\n",
    "# 1) Declare with a signature, and pass some config.\n",
    "classify = dspy.ChainOfThought('question -> answer')\n",
    "\n",
    "# 2) Call with input argument.\n",
    "response = classify(question=question)\n",
    "\n",
    "# 3) Access the outputs.\n",
    "response.completions.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: The ColBERT retrieval model is known for its effectiveness in retrieving relevant documents from large collections of text. One great aspect of ColBERT is its ability to leverage semantic search, which allows it to find documents that are semantically similar to the query, rather than just matching keywords.\n",
      "Answer: ColBERT's use of semantic search enables more accurate and informative retrieval results, making it a valuable tool for applications such as information retrieval, question answering, and text classification.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Reasoning: {response.reasoning}\")\n",
    "print(f\"Answer: {response.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The others are very similar. They mainly change the internal behavior with which your signature is implemented!\n",
    "\n",
    "1. `dspy.Predict`: Basic predictor. Does not modify the signature. Handles the key forms of learning (i.e., storing the instructions and demonstrations and updates to the LM).\n",
    "\n",
    "2. `dspy.ChainOfThought`. : Teaches the LM to think step-by-step before committing to the signature's response.\n",
    "\n",
    "3. `dspy.ProgramOfThought`: Teaches the LM to output code, whose execution results will dictate the response.\n",
    "\n",
    "4. `dspy.ReAct`: An agent that can use tools to implement the given signature.\n",
    "\n",
    "5. `dspy.MultiChainComparison`: Can compare multiple outputs from ChainOfThought to produce a final prediction.\n",
    "\n",
    "6. `dspy.majority`: Can do basic voting to return the most popular response from a set of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning='To calculate the probability, we need to find all possible outcomes where the sum equals two. The only way this can happen is if both dice show a 1. There are 6 possible outcomes when rolling two dice (1,1), (1,2), (1,3), (1,4), (1,5), and (1,6). Only one of these outcomes has a sum of two. Therefore, the probability is 1/6.',\n",
       "    answer=0.16666666666666666\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Math example\n",
    "math = dspy.ChainOfThought('question -> answer: float')\n",
    "math(question=\"Two dice are tossed. What is the probability that the sum equals two?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning='The text mentions that David Gregory inherited Kinnairdy Castle in 1664.',\n",
       "    response='Kinnairdy Castle'\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieval-Argumanted Generation Example\n",
    "def search(query: str) -> List[str]:\n",
    "    \"\"\"Retrieves abstracts from wikipedia.\"\"\"\n",
    "    results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3)\n",
    "    return [x['text'] for x in results]\n",
    "\n",
    "rag = dspy.ChainOfThought('context, question -> response')\n",
    "question = \"What's the name of the castle that David Gregory inherited?\"\n",
    "rag(context=search(question), question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    sentiment='neutral',\n",
       "    confidence=0.75\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification Example\n",
    "from typing import Literal\n",
    "\n",
    "class Classify(dspy.Signature):\n",
    "    \"\"\"Classify sentiment of a given sentence.\"\"\"\n",
    "\n",
    "    sentence: str = dspy.InputField()\n",
    "    sentiment: Literal['positive', 'negative', 'neutral'] = dspy.OutputField()\n",
    "    confidence: float = dspy.OutputField()\n",
    "\n",
    "classify = dspy.Predict(Classify)\n",
    "classify(sentence=\"This book was super fun to read, though not the last chapter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone 14 Announcement\n",
      "['New iPhone Features', 'Press Release']\n",
      "[{'key': 'Company', 'value': 'Apple Inc.'}, {'key': 'Product', 'value': 'iPhone 14'}, {'key': 'CEO', 'value': 'Tim Cook'}]\n"
     ]
    }
   ],
   "source": [
    "#Information Extraction Example\n",
    "text = \"Apple Inc. announced its latest iPhone 14 today. The CEO, Tim Cook, highlighted its new features in a press release.\"\n",
    "\n",
    "module = dspy.Predict(\"text -> title, headings: list[str], entities_and_metadata: list[dict[str, str]]\")\n",
    "response = module(text=text)\n",
    "\n",
    "print(response.title)\n",
    "print(response.headings)\n",
    "print(response.entities_and_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9361.2158\n"
     ]
    }
   ],
   "source": [
    "#Agent example\n",
    "def evaluate_math(expression: str) -> float:\n",
    "    return dspy.PythonInterpreter({}).execute(expression)\n",
    "\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3)\n",
    "    return [x['text'] for x in results]\n",
    "\n",
    "react = dspy.ReAct(\"question -> answer: float\", tools=[evaluate_math, search_wikipedia])\n",
    "\n",
    "pred = react(question=\"What is 9362158 divided by the year of birth of David Gregory of Kinnairdy castle?\")\n",
    "print(pred.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "SPy is a machine learning framework, so working in it involves training sets, development sets, and test sets. For each example in your data, we distinguish typically between three types of values: the inputs, the intermediate labels, and the final label. You can use DSPy effectively without any intermediate or final labels, but you will need at least a few example inputs.\n",
    "\n",
    "### DSPy Example Objects\n",
    "The core data type for data in DSPy is `Example`. You will use Examples to represent items in your training set and test set.\n",
    "\n",
    "DSPy **Examples** are similar to Python dicts but have a few useful utilities. Your DSPy modules will return values of the type `Prediction`, which is a special sub-class of Example.\n",
    "\n",
    "When you use DSPy, you will do a lot of evaluation and optimization runs. Your individual datapoints will be of type Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'question': 'This is a question?', 'answer': 'This is an answer.'}) (input_keys=None)\n",
      "This is a question?\n",
      "This is an answer.\n"
     ]
    }
   ],
   "source": [
    "qa_pair = dspy.Example(question=\"This is a question?\", answer=\"This is an answer.\")\n",
    "\n",
    "print(qa_pair)\n",
    "print(qa_pair.question)\n",
    "print(qa_pair.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples can have any field keys and any value types, though usually values are strings.\n",
    "\n",
    "`object = Example(field1=value1, field2=value2, field3=value3, ...)`\n",
    "\n",
    "You can now express your training set for example as:\n",
    "\n",
    "`trainset = [dspy.Example(report=\"LONG REPORT 1\", summary=\"short summary 1\"), ...]`\n",
    "\n",
    "#### Specifying Input Keys\n",
    "In DSPy, the `Example` objects have a `with_inputs()` method, which can mark specific fields as inputs. (The rest are just metadata or labels.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'question': 'This is a question?', 'answer': 'This is an answer.'}) (input_keys={'question'})\n",
      "Example({'question': 'This is a question?', 'answer': 'This is an answer.'}) (input_keys={'answer', 'question'})\n"
     ]
    }
   ],
   "source": [
    "# Single Input.\n",
    "print(qa_pair.with_inputs(\"question\"))\n",
    "\n",
    "# Multiple Inputs; be careful about marking your labels as inputs unless you mean it.\n",
    "print(qa_pair.with_inputs(\"question\", \"answer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values can be accessed using the .(dot) operator. You can access the value of key name in defined object `Example(name=\"John Doe\", job=\"sleep\")` through `object.name`.\n",
    "\n",
    "To access or exclude certain keys, use `inputs()` and `labels()` methods to return new Example objects containing only input or non-input keys, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example object with Input fields only: Example({'article': 'This is an article.'}) (input_keys={'article'})\n",
      "Example object with Non-Input fields only: Example({'summary': 'This is a summary.'}) (input_keys=None)\n"
     ]
    }
   ],
   "source": [
    "article_summary = dspy.Example(article= \"This is an article.\", summary= \"This is a summary.\").with_inputs(\"article\")\n",
    "\n",
    "input_key_only = article_summary.inputs()\n",
    "non_input_key_only = article_summary.labels()\n",
    "\n",
    "print(\"Example object with Input fields only:\", input_key_only)\n",
    "print(\"Example object with Non-Input fields only:\", non_input_key_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "DSPy is a machine learning framework, so you must think about your automatic metrics for evaluation (to track your progress) and optimization (so DSPy can make your programs more effective).\n",
    "\n",
    "#### What is a metric and how do I define a metric for my task?\n",
    "A metric is just a function that will take examples from your data and the output of your system and return a score that quantifies how good the output is. What makes outputs from your system good or bad?\n",
    "\n",
    "For simple tasks, this could be just \"accuracy\" or \"exact match\" or \"F1 score\". This may be the case for simple classification or short-form QA tasks.\n",
    "\n",
    "However, for most applications, your system will output long-form outputs. There, your metric should probably be a smaller DSPy program that checks multiple properties of the output (quite possibly using AI feedback from LMs).\n",
    "\n",
    "Getting this right on the first try is unlikely, but you should start with something simple and iterate.\n",
    "\n",
    "#### Simple metrics\n",
    "A DSPy metric is just a function in Python that takes example (e.g., from your training or dev set) and the output pred from your DSPy program, and outputs a float (or int or bool) score.\n",
    "\n",
    "Your metric should also accept an optional third argument called trace. You can ignore this for a moment, but it will enable some powerful tricks if you want to use your metric for optimization.\n",
    "\n",
    "Here's a simple example of a metric that's comparing example.answer and pred.answer. This particular metric will return a bool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_answer(example, pred, trace=None):\n",
    "    return example.answer.lower() == pred.answer.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some people find these utilities (built-in) convenient:\n",
    "\n",
    "* `dspy.evaluate.metrics.answer_exact_match`\n",
    "* `dspy.evaluate.metrics.answer_passage_match`\n",
    "\n",
    "Your metrics could be more complex, e.g. check for multiple properties. The metric below will return a `float `if `trace` is None (i.e., if it's used for evaluation or optimization), and will return a `bool` otherwise (i.e., if it's used to bootstrap demonstrations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    # check the gold label and the predicted answer are the same\n",
    "    answer_match = example.answer.lower() == pred.answer.lower()\n",
    "\n",
    "    # check the predicted answer comes from one of the retrieved contexts\n",
    "    context_match = any((pred.answer.lower() in c) for c in pred.context)\n",
    "\n",
    "    if trace is None: # if we're doing evaluation or optimization\n",
    "        return (answer_match + context_match) / 2.0\n",
    "    else: # if we're doing bootstrapping, i.e. self-generating good demonstrations of each step\n",
    "        return answer_match and context_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "Once you have a metric, you can run evaluations in a simple Python loop.\n",
    "\n",
    "```python\n",
    "scores = []\n",
    "for x in devset:\n",
    "    pred = program(**x.inputs())\n",
    "    score = metric(x, pred)\n",
    "    scores.append(score)\n",
    "\n",
    "```\n",
    "\n",
    "If you need some utilities, you can also use the built-in Evaluate utility. It can help with things like parallel evaluation (multiple threads) or showing you a sample of inputs/outputs and the metric scores.\n",
    "\n",
    "```python\n",
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "# Set up the evaluator, which can be re-used in your code.\n",
    "evaluator = Evaluate(devset=YOUR_DEVSET, num_threads=1, display_progress=True, display_table=5)\n",
    "\n",
    "# Launch evaluation.\n",
    "evaluator(YOUR_PROGRAM, metric=YOUR_METRIC)\n",
    "```\n",
    "\n",
    "#### Intermediate: Using AI feedback for your metric\n",
    "For most applications, your system will output long-form outputs, so your metric should check multiple dimensions of the output using AI feedback from LMs.\n",
    "\n",
    "This simple signature could come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the signature for automatic assessments.\n",
    "class Assess(dspy.Signature):\n",
    "    \"\"\"Assess the quality of a tweet along the specified dimension.\"\"\"\n",
    "\n",
    "    assessed_text = dspy.InputField()\n",
    "    assessment_question = dspy.InputField()\n",
    "    assessment_answer: bool = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, below is a simple metric that checks a generated tweet (1) answers a given question correctly and (2) whether it's also engaging. We also check that (3) len(tweet) <= 280 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(gold, pred, trace=None):\n",
    "    question, answer, tweet = gold.question, gold.answer, pred.output\n",
    "\n",
    "    engaging = \"Does the assessed text make for a self-contained, engaging tweet?\"\n",
    "    correct = f\"The text should answer `{question}` with `{answer}`. Does the assessed text contain this answer?\"\n",
    "\n",
    "    correct =  dspy.Predict(Assess)(assessed_text=tweet, assessment_question=correct)\n",
    "    engaging = dspy.Predict(Assess)(assessed_text=tweet, assessment_question=engaging)\n",
    "\n",
    "    correct, engaging = [m.assessment_answer for m in [correct, engaging]]\n",
    "    score = (correct + engaging) if correct and (len(tweet) <= 280) else 0\n",
    "\n",
    "    if trace is not None: return score >= 2\n",
    "    return score / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When compiling, `trace` is not None, and we want to be strict about judging things, so we will only return True if score >= 2. Otherwise, we return a score out of 1.0 (i.e., score / 2.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSPy Optimizers (formerly Teleprompters)\n",
    "A DSPy optimizer is an algorithm that can tune the parameters of a DSPy program (i.e., the prompts and/or the LM weights) to maximize the metrics you specify, like accuracy.\n",
    "\n",
    "A typical DSPy optimizer takes three things:\n",
    "\n",
    "* Your **DSPy program**. This may be a single module (e.g., `dspy.Predict`) or a complex multi-module program.\n",
    "\n",
    "* Your **metric***. This is a function that evaluates the output of your program, and assigns it a score (higher is better).\n",
    "\n",
    "* A few **training inputs**. This may be very small (i.e., only 5 or 10 examples) and incomplete (only inputs to your program, without any labels).\n",
    "\n",
    "#### What does a DSPy Optimizer tune? How does it tune them?\n",
    "Different optimizers in DSPy will tune your program's quality by **synthesizing good few-shot examples** for every module,\n",
    "\n",
    " like `dspy.BootstrapRS`, **proposing and intelligently exploring better natural-language instructions** for every prompt, \n",
    " \n",
    " like `dspy.MIPROv2`, and **building datasets for your modules and using them to finetune the LM weights** in your system, \n",
    " \n",
    " like `dspy.BootstrapFinetune`.\n",
    "\n",
    "\n",
    "#### What DSPy Optimizers are currently available?\n",
    "\n",
    "Optimizers can be accessed via from `dspy.teleprompt import *`.\n",
    "\n",
    "These optimizers extend the signature by automatically generating and including optimized examples within the prompt sent to the model, implementing few-shot learning.\n",
    "\n",
    "* **LabeledFewShot**: Simply constructs few-shot examples (demos) from provided labeled input and output data points. Requires k (number of examples for the prompt) and trainset to randomly select k examples from.\n",
    "\n",
    "* **BootstrapFewShot**: Uses a teacher module (which defaults to your program) to generate complete demonstrations for every stage of your program, along with labeled examples in trainset. Parameters include max_labeled_demos (the number of demonstrations randomly selected from the trainset) and max_bootstrapped_demos (the number of additional examples generated by the teacher). The bootstrapping process employs the metric to validate demonstrations, including only those that pass the metric in the \"compiled\" prompt. Advanced: Supports using a teacher program that is a different DSPy program that has compatible structure, for harder tasks.\n",
    "\n",
    "* **BootstrapFewShotWithRandomSearch**: Applies BootstrapFewShot several times with random search over generated demonstrations, and selects the best program over the optimization. Parameters mirror those of BootstrapFewShot, with the addition of num_candidate_programs, which specifies the number of random programs evaluated over the optimization, including candidates of the uncompiled program, LabeledFewShot optimized program, BootstrapFewShot compiled program with unshuffled examples and num_candidate_programs of BootstrapFewShot compiled programs with randomized example sets.\n",
    "\n",
    "* **KNNFewShot**. Uses k-Nearest Neighbors algorithm to find the nearest training example demonstrations for a given input example. These nearest neighbor demonstrations are then used as the trainset for the BootstrapFewShot optimization process. See this notebook for an example.\n",
    "\n",
    "#### Automatic Instruction Optimization\n",
    "These optimizers produce optimal instructions for the prompt and, in the case of MIPROv2 can also optimize the set of few-shot demonstrations.\n",
    "\n",
    "* **COPRO**: Generates and refines new instructions for each step, and optimizes them with coordinate ascent (hill-climbing using the metric function and the trainset). Parameters include depth which is the number of iterations of prompt improvement the optimizer runs over.\n",
    "\n",
    "* **MIPROv2**: Generates instructions and few-shot examples in each step. The instruction generation is data-aware and demonstration-aware. Uses Bayesian Optimization to effectively search over the space of generation instructions/demonstrations across your modules.\n",
    "\n",
    "#### Automatic Finetuning\n",
    "\n",
    "This optimizer is used to fine-tune the underlying LLM(s).\n",
    "\n",
    "* **BootstrapFinetune**: Distills a prompt-based DSPy program into weight updates. The output is a DSPy program that has the same steps, but where each step is conducted by a finetuned model instead of a prompted LM.\n",
    "\n",
    "#### Program Transformations\n",
    "* **Ensemble**: Ensembles a set of DSPy programs and either uses the full set or randomly samples a subset into a single program.\n",
    "\n",
    "#### Which optimizer should I use?\n",
    "Ultimately, finding the ‘right’ optimizer to use & the best configuration for your task will require experimentation. Success in DSPy is still an iterative process - getting the best performance on your task will require you to explore and iterate.\n",
    "\n",
    "That being said, here's the general guidance on getting started:\n",
    "\n",
    "* If you have **very few examples** (around 10), start with `BootstrapFewShot`.\n",
    "* If you have **more data** (50 examples or more), try `BootstrapFewShotWithRandomSearch`.\n",
    "* If you prefer to do **instruction optimization only** (i.e. you want to keep your prompt 0-shot), use `MIPROv2` configured for 0-shot optimization to optimize.\n",
    "* If you’re willing to use more inference calls to perform **longer optimization** runs (e.g. 40 trials or more), and have enough data (e.g. 200 examples or more to prevent overfitting) then try `MIPROv2`.\n",
    "* If you have been able to use one of these with a large LM (e.g., 7B parameters or above) and need a very **efficient program**, finetune a small LM for your task with `BootstrapFinetune`.\n",
    "\n",
    "```python\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "# Set up the optimizer: we want to \"bootstrap\" (i.e., self-generate) 8-shot examples of your program's steps.\n",
    "# The optimizer will repeat this 10 times (plus some initial attempts) before selecting its best attempt on the devset.\n",
    "config = dict(max_bootstrapped_demos=4, max_labeled_demos=4, num_candidate_programs=10, num_threads=4)\n",
    "\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(metric=YOUR_METRIC_HERE, **config)\n",
    "optimized_program = teleprompter.compile(YOUR_PROGRAM_HERE, trainset=YOUR_TRAINSET_HERE)\n",
    "```\n",
    "\n",
    "#### Saving and loading optimizer output\n",
    "After running a program through an optimizer, it's useful to also save it. At a later point, a program can be loaded from a file and used for inference. For this, the load and save methods can be used.\n",
    "\n",
    "`optimized_program.save(YOUR_SAVE_PATH)`\n",
    "\n",
    "To load a program from a file, you can instantiate an object from that class and then call the load method on it.\n",
    "\n",
    "```python\n",
    "loaded_program = YOUR_PROGRAM_CLASS()\n",
    "loaded_program.load(path=YOUR_SAVE_PATH)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/27 15:21:41 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "RUNNING WITH THE FOLLOWING LIGHT AUTO RUN SETTINGS:\n",
      "num_trials: 7\n",
      "minibatch: True\n",
      "num_candidates: 3\n",
      "valset size: 100\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m react \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mReAct(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion -> answer\u001b[39m\u001b[38;5;124m\"\u001b[39m, tools\u001b[38;5;241m=\u001b[39m[search])\n\u001b[1;32m     15\u001b[0m tp \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mMIPROv2(metric\u001b[38;5;241m=\u001b[39mdspy\u001b[38;5;241m.\u001b[39mevaluate\u001b[38;5;241m.\u001b[39manswer_exact_match, auto\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlight\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m optimized_react \u001b[38;5;241m=\u001b[39m \u001b[43mtp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/llmenv/lib/python3.11/site-packages/dspy/teleprompt/mipro_optimizer_v2.py:145\u001b[0m, in \u001b[0;36mMIPROv2.compile\u001b[0;34m(self, student, trainset, teacher, valset, num_trials, max_bootstrapped_demos, max_labeled_demos, seed, minibatch, minibatch_size, minibatch_full_eval_steps, program_aware_proposer, data_aware_proposer, view_data_batch_size, tip_aware_proposer, fewshot_aware_proposer, requires_permission_to_run)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Estimate LM calls and get user confirmation\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m requires_permission_to_run:\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_user_confirmation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminibatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminibatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminibatch_full_eval_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogram_aware_proposer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    154\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompilation aborted by the user.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m student  \u001b[38;5;66;03m# Return the original student program\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/llmenv/lib/python3.11/site-packages/dspy/teleprompt/mipro_optimizer_v2.py:365\u001b[0m, in \u001b[0;36mMIPROv2._get_user_confirmation\u001b[0;34m(self, program, num_trials, minibatch, minibatch_size, minibatch_full_eval_steps, valset, program_aware_proposer)\u001b[0m\n\u001b[1;32m    333\u001b[0m user_message \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mYELLOW\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mBOLD\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mProjected Language Model (LM) Calls\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENDC\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mYELLOW\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m- Setting `minibatch=True` if you haven\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt already.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENDC\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    353\u001b[0m )\n\u001b[1;32m    355\u001b[0m user_confirmation_message \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124m    To proceed with the execution of this program, please confirm by typing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBLUE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENDC\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for yes or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBLUE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENDC\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for no.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    363\u001b[0m )\n\u001b[0;32m--> 365\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43muser_message\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43muser_confirmation_message\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDo you wish to continue? (y/n): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m user_input \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/llmenv/lib/python3.11/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/llmenv/lib/python3.11/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Optimizing prompts for a ReAct agent\n",
    "import dspy\n",
    "from dspy.datasets import HotPotQA\n",
    "\n",
    "dspy.configure(lm=dspy.LM('ollama_chat/llama3.3', api_base='http://localhost:11434'))\n",
    "\n",
    "def search(query: str) -> list[str]:\n",
    "    \"\"\"Retrieves abstracts from Wikipedia.\"\"\"\n",
    "    results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3)\n",
    "    return [x['text'] for x in results]\n",
    "\n",
    "trainset = [x.with_inputs('question') for x in HotPotQA(train_seed=2024, train_size=500).train]\n",
    "react = dspy.ReAct(\"question -> answer\", tools=[search])\n",
    "\n",
    "tp = dspy.MIPROv2(metric=dspy.evaluate.answer_exact_match, auto=\"light\", num_threads=24)\n",
    "optimized_react = tp.compile(react, trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing prompts for RAG\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_docs=5):\n",
    "        self.num_docs = num_docs\n",
    "        self.respond = dspy.ChainOfThought('context, question -> response')\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = search(question, k=self.num_docs)   # not defined in this snippet, see link above\n",
    "        return self.respond(context=context, question=question)\n",
    "\n",
    "tp = dspy.MIPROv2(metric=dspy.SemanticF1(), auto=\"medium\", num_threads=24)\n",
    "optimized_rag = tp.compile(RAG(), trainset=trainset, max_bootstrapped_demos=2, max_labeled_demos=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing weights for Classification\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_docs=5):\n",
    "        self.num_docs = num_docs\n",
    "        self.respond = dspy.ChainOfThought('context, question -> response')\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = search(question, k=self.num_docs)   # not defined in this snippet, see link above\n",
    "        return self.respond(context=context, question=question)\n",
    "\n",
    "tp = dspy.MIPROv2(metric=dspy.SemanticF1(), auto=\"medium\", num_threads=24)\n",
    "optimized_rag = tp.compile(RAG(), trainset=trainset, max_bootstrapped_demos=2, max_labeled_demos=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
